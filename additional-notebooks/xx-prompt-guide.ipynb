{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80217267-554b-477e-9348-265de77e6325",
   "metadata": {},
   "source": [
    "# Image Prompt Playground\n",
    "\n",
    "The [Prerequisites](00-prerequisites.ipynb), [Video segments: frames, shots and scenes](01A-visual-segments-frames-shots-scenes.ipynb) and [Ad Breaks and Contextual Ad Targeting](02-ad-breaks-and-contextual-ad-targeting.ipynb) notebooks are prerequisites for this prompting exercise.\n",
    "\n",
    "In the Ad break detection and contextual Ad targeting notebook, we assembled video frames associated with topics and created composite image grids.  You can go back to that notebook and look at the section titled \"Generate chapter level contextual information\" for a visual description of the flow.  \n",
    "\n",
    "Here, we are going to walk you through the prompt we generate using shots and show you how Claude responds based on the prompting.\n",
    "\n",
    "First let's load all the necessary code and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6752b0d9-ee74-46ba-9556-575b028c8ac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import json_repair\n",
    "import copy\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "#from lib.chapters import Chapters\n",
    "from lib import frame_utils\n",
    "from lib import util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "import copy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137d5cd-4647-40f1-98c3-b804bad7a4d5",
   "metadata": {},
   "source": [
    "# Supporting code\n",
    "\n",
    "Below you will find a number of methods we pulled out of our [bedrock_helper](./lib/bedrock_helper.py) python module.  We've done this to expose the prompts so that you can see how your prompts impact the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6656cd36-fea9-41d2-83d0-eff354882a78",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Chapters:\n",
    "    def __init__(self, topics, scenes, frames):\n",
    "        self.video_asset_dir = frames.video_asset_dir()\n",
    "        self.chapters = self.align_scenes_in_chapters(topics, scenes, frames)\n",
    "        \n",
    "    def align_scenes_in_chapters(self, topics, scenes, frames):\n",
    "        \"\"\"\n",
    "        Aligns video scenes with conversation topics to create chronological chapters.\n",
    "    \n",
    "        Args:\n",
    "            topics: List of conversation topics with start_ms, end_ms, and reason\n",
    "            scenes: List of scene metadata with start_ms and end_ms\n",
    "            frames: List of video frame metadata\n",
    "    \n",
    "        Returns:\n",
    "            List of chapters, each containing aligned scenes and associated text\n",
    "    \n",
    "        Note:\n",
    "            - Handles scenes without conversations\n",
    "            - Merges overlapping topics\n",
    "            - Preserves chronological order\n",
    "            - Creates Chapter objects for each segment\n",
    "    \"\"\"\n",
    "        scenes = copy.deepcopy(scenes)\n",
    "    \n",
    "        chapters = []\n",
    "        for topic in topics:\n",
    "            \n",
    "            topic_start_ms = topic['start_ms']\n",
    "            topic_end_ms = topic['end_ms']\n",
    "            text = topic['reason']\n",
    "\n",
    "            # find all the frames that align with the conversation topic\n",
    "            stack = []\n",
    "            while len(scenes) > 0:\n",
    "                scene = scenes[0]\n",
    "                frame_start = scene['start_ms']\n",
    "                frame_end = scene['end_ms']\n",
    "\n",
    "                \n",
    "                if frame_start > topic_end_ms:\n",
    "                    # topic overlaps scenes that belong to previous topic - merge the text\n",
    "                    if not stack:\n",
    "                        num_chapters = len(chapters)\n",
    "                        if num_chapters > 0:\n",
    "                            chapters[num_chapters-1]['text'] = chapters[num_chapters-1]['text'] + ' ' + text\n",
    "                        \n",
    "                    break\n",
    "    \n",
    "                # scenes before any conversation starts\n",
    "                if frame_end < topic_start_ms:\n",
    "                    chapter = Chapter(len(chapters), [scene], frames).__dict__\n",
    "                    chapters.append(chapter)\n",
    "                    scenes.pop(0)\n",
    "                    continue\n",
    "    \n",
    "                stack.append(scene)\n",
    "                scenes.pop(0)\n",
    "    \n",
    "            if stack:\n",
    "                chapter = Chapter(len(chapters), stack, frames, text).__dict__\n",
    "                chapters.append(chapter)\n",
    "    \n",
    "        ## There could be more scenes without converations, append them\n",
    "        for scene in scenes:\n",
    "            chapter = Chapter(len(chapters), [scene], frames).__dict__\n",
    "            chapters.append(chapter)\n",
    "    \n",
    "        return chapters\n",
    "\n",
    "class Chapter:\n",
    "    def __init__(self, chapter_id, scenes, frames, text = ''):\n",
    "        self.scene_ids = [scene['id'] for scene in scenes]\n",
    "        self.start_frame_id = scenes[0]['start_frame_id']\n",
    "        self.end_frame_id = scenes[-1]['end_frame_id']\n",
    "        self.start_ms = scenes[0]['start_ms']\n",
    "        self.end_ms = scenes[-1]['end_ms']\n",
    "        self.id = chapter_id\n",
    "        self.text = text\n",
    "        #folder = os.path.join(frames.video_asset_dir(), 'chapters')\n",
    "        #os.makedirs(folder, exist_ok=True) \n",
    "        self.composite_images = frames.create_composite_images(frames.frames[self.start_frame_id:self.end_frame_id+1], 'chapters', prefix=\"chapter_\")\n",
    "        \n",
    "        return \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de7dfebd-aab1-4e33-9478-39d978284974",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0c3380c-e1c6-4962-bc91-b43c0240f52b",
   "metadata": {},
   "source": [
    "# Encoding the images for Claude\n",
    "\n",
    "The method make_image_message() will take a number of images and base64 encode them so we can send them to Claude for understanding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc37378-9034-4743-b23e-880bc3139a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# encode the images along with a prompt so that we can get some context about our frames or scenes from the LLM\n",
    "#\n",
    "def make_image_message(composite_images):\n",
    "    # adding the composite image sequences\n",
    "    image_contents = [{\n",
    "        'type': 'text',\n",
    "        'text': 'Here are {0} images containing a frame sequence that describes a scene.'.format(len(composite_images))\n",
    "    }]\n",
    "\n",
    "    open_images = []\n",
    "    for image in composite_images:\n",
    "        with open(image['file'], \"rb\") as image_file:\n",
    "            image_data = image_file.read()\n",
    "            open_images.append(image_file)\n",
    "        image_pil = Image.open(BytesIO(image_data))\n",
    "        bas64_image = frame_utils.image_to_base64(image_pil)\n",
    "        image_contents.append({\n",
    "            'type': 'image',\n",
    "            'source': {\n",
    "                'type': 'base64',\n",
    "                'media_type': 'image/jpeg',\n",
    "                'data': bas64_image\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # close the images\n",
    "    for image in open_images:\n",
    "        image.close()\n",
    "\n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': image_contents\n",
    "    }\n",
    "\n",
    "\n",
    "#\n",
    "# get rid of the encoded image data which results in a very long output message and just clutters things up\n",
    "#\n",
    "def remove_data_field(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: remove_data_field(v) if k != 'data' else '<snip>' for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_data_field(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "\n",
    "def make_output_example():\n",
    "    example = {\n",
    "        'description': {\n",
    "            'text': 'The scene describes...',\n",
    "            'score': 98\n",
    "        },\n",
    "        'sentiment': {\n",
    "            'text': 'Positive',\n",
    "            'score': 90\n",
    "        },\n",
    "        'genre': {\n",
    "            'text': 'True Crime',\n",
    "            'score': 80\n",
    "        },\n",
    "        'relevant_tags': [\n",
    "            {\n",
    "                'text': 'BMW',\n",
    "                'score': 95\n",
    "            }\n",
    "        ]            \n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': 'Return JSON format. An example of the output:\\n{0}\\n'.format(json.dumps(example))\n",
    "    }\n",
    "\n",
    "\n",
    "#\n",
    "# use this method to show what's being passed to the LLM.  It will strip out the encoded image\n",
    "# information that clutters up the output. \n",
    "#\n",
    "def show_llm_messages(model_params):\n",
    "    print(f'\\nMessages:\\n')\n",
    "    for message in model_params:\n",
    "        try:\n",
    "            cleaned_message = remove_data_field(message)\n",
    "            json_string = json.dumps(cleaned_message, indent=2)\n",
    "            print(json_string)\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding message: {e}\")\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425fca6d-3967-4a15-87fd-d5e94c8130dd",
   "metadata": {},
   "source": [
    "# Get Contextual Information\n",
    "\n",
    "Below you find the get_contextual_information() method that is in our [bedrock_helper](./lib/bedrock_helper.py) Python module.  We've made some modifications so that you can pass a variety of system and user prompts and observe the effect on outputs from Claude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba308d-32d2-4ea7-81c9-a56f7c22fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lib import bedrock_helper as brh\n",
    "\n",
    "#\n",
    "# This is the meat of the code that builds up the conversation to pass to the LLM.\n",
    "#\n",
    "def get_contextual_information(images, system_prompt, user_prompt):\n",
    "    messages = []\n",
    "    # adding sequences of composite images to the prompt.  Limit is 20.\n",
    "    message_images = make_image_message(images[:19])\n",
    "    messages.append(message_images)\n",
    "\n",
    "    # other information\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. Do you have other information to provdie?'\n",
    "    })\n",
    "\n",
    "    other_information = []\n",
    "\n",
    "    ## Sentiment\n",
    "    sentiment_list = brh.make_sentiments()\n",
    "    other_information.append(sentiment_list)\n",
    "\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': other_information\n",
    "    })\n",
    "\n",
    "    # output format\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. What output format?'\n",
    "    })\n",
    "    output_format = make_output_example()\n",
    "    messages.append(output_format)\n",
    "\n",
    "    # prefill '{'\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })    \n",
    "    formatted_system_prompt = system_prompt.format(user_prompt)\n",
    "    model_params = {\n",
    "        'anthropic_version': brh.MODEL_VER,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': formatted_system_prompt,\n",
    "        'messages': messages\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = brh.inference(model_params)\n",
    "    except Exception as e:\n",
    "        print(colored(f\"ERR: inference: {str(e)}\\n RETRY...\", 'red'))\n",
    "        response = inference(model_params)\n",
    "\n",
    "    return formatted_system_prompt, messages, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ea79e-8c0d-4846-9783-0c441e06c348",
   "metadata": {},
   "source": [
    "# Our main method\n",
    "\n",
    "Below is the code we will use to display the image we are passing to Claude, call get_contextual_information() to have the actual conversation with the model and then display the cost of the conversation.  This is the main method for testing different prompts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698403eb-0715-4304-a37e-715a70288d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_llm(image_list, system_prompt, user_prompt):\n",
    "\n",
    "    total_usage = {\n",
    "        'input_tokens': 0,\n",
    "        'output_tokens': 0,\n",
    "    }\n",
    "    \n",
    "    for idx, composite_image in enumerate(image_list):\n",
    "        print (f'\\nImage {idx+1 } of { len(image_list) }: { composite_image[\"file\"] }\\n')\n",
    "        display(DisplayImage(filename=composite_image['file']))\n",
    "    \n",
    "    conversation_text = ''\n",
    "    \n",
    "    print(\"\\nSending conversation to LLM ...\")\n",
    "    system_prompt, messages, contextual_response = get_contextual_information(image_list, system_prompt, user_prompt)\n",
    "    print(\"Got a response\\n\")\n",
    "    \n",
    "    usage = contextual_response['usage']\n",
    "    contextual = contextual_response['content'][0]['json']\n",
    "    \n",
    "    total_usage['input_tokens'] += usage['input_tokens']\n",
    "    total_usage['output_tokens'] += usage['output_tokens']\n",
    "    \n",
    "    for key in ['description', 'sentiment']:\n",
    "        print(f\"{key.capitalize()}: {colored(contextual[key]['text'], 'green')} ({contextual[key]['score']}%)\")\n",
    "    \n",
    "    for key in ['relevant_tags']:\n",
    "        try:\n",
    "            items = ', '.join([item['text'] for item in contextual[key]])\n",
    "            if len(items) == 0:\n",
    "                items = 'None'\n",
    "            print(f\"{key.capitalize()}: {colored(items, 'green')}\")\n",
    "        except KeyError:\n",
    "            pass\n",
    "    print(f\"================================================\")\n",
    "    \n",
    "    contextual_cost = brh.display_contextual_cost(total_usage)\n",
    "\n",
    "    return system_prompt, messages, contextual_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfa43d-3553-45a3-9536-58e34f613800",
   "metadata": {},
   "source": [
    "# Let's do some setup before we talk to Claude\n",
    "\n",
    "Below we define our system prompt as well as an image list for which we want contextual information.   The system prompt is the foundational model's role in gathering the contextual information and instructions on how to interpret the input. We are going to leave this alone.  The user prompt is what you will change for different tasks.  For example you may want to get a summarization of a shot, or classify the scene and get the genre.  This first prompt is going to be somewhat detaild and include tags, brands and logos.\n",
    "\n",
    "We are using the video['shots'] structure to access the shots we generated in the previous notebook.  If you'd like to look at the various shots, you can look in the folder we print out below.  We also display the JSON video shots dictionary entry where you can see the composite_images that we pass to Claude.  Each shot is a single image made up of multiple frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2e74e-03ae-4802-9600-04b3fd174a3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = 'You are asked to provide the following information: a detail description to describe the scene, identify sentiment, brands and logos that may appear in the scene, and five most relevant tags from the scene.'\n",
    "system_prompt = 'You are a media operation engineer. Your job is to review a portion of a video content presented in a sequence of consecutive images. Each image may also contain a sequence of frames presented in a 4x7 grid reading from left to right and then from top to bottom. {0} It is important to return the results in JSON format and also include a confidence score from 0 to 100. Skip any explanation.'\n",
    "\n",
    "\n",
    "print(\"\\nThe images we will be using are stored in\", \"./\"+video[\"output_dir\"]+\"/shots/\\n\")\n",
    "\n",
    "idx = 4\n",
    "print(\"Shot\", \"#\"+str(idx+1))\n",
    "shot = video['shots'].shots[idx]\n",
    "\n",
    "\n",
    "display(\"video['shots'].shots[idx] JSON ->\", JSON(shot))\n",
    "image_list = [ { 'file' :shot['composite_images'][0]['file'] } ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96662f5c-b678-4b80-94e6-e84cdedfb67d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Call Claude via the Bedrock API\n",
    "#\n",
    "system_prompt, messages, contextual_response = prompt_llm(image_list, system_prompt, user_prompt)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6053092c-de70-44d2-9a8c-774dc126fffd",
   "metadata": {},
   "source": [
    "# Conversation with Claude using Bedrock\n",
    "\n",
    "Let's take a look at the conversation we've sent to Claude via Bedrock.  We are using the [Anthropic Claude Messages API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html).\n",
    "\n",
    "We've stripped out the base64 encoded image(s) for display purposes and replaced them with \"\\<snip\\>\".   The \"user\" messages are from the user (you) and the \"assistant\" is Claude.  You will get a response to your prompt along with the calculated cost from the Bedrock call.  We also provide the JSON messages that we sent via the API so you can see how the conversation is constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb908d-4f64-4a78-a90f-b6fc0ed2db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "display(\"user_prompt:\", user_prompt)\n",
    "print(\"\\n\")\n",
    "display(\"system_prompt:\", system_prompt)\n",
    "print(\"\\n\")\n",
    "display(\"messages\", JSON(remove_data_field(messages), expanded=1))\n",
    "print(\"\\n\")\n",
    "display(\"contextual_response:\", JSON(contextual_response, expanded=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ffacdd-6b15-41b8-a119-95c16bb315ec",
   "metadata": {},
   "source": [
    "# New prompt - Classification\n",
    "\n",
    "Let's try changing the prompt and see what we get.  This time we will ask just about the genre and ask Claude to give that in the description.  We've unraveled the contextual_response JSON to show you the genre tag that now shows up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df2413-60ec-40bb-bcc8-aeaabe5bff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = 'Please give the genre of the movie.'\n",
    "system_prompt = 'You are a media operation engineer. Your job is to review a portion of a video content presented in a sequence of consecutive images. Each image may also contain a sequence of frames presented in a 4x7 grid reading from left to right and then from top to bottom. {0} It is important to return the results in JSON format and also include a confidence score from 0 to 100. Skip any explanation.'\n",
    "\n",
    "\n",
    "# media-analysis-with-generative-ai-on-aws/Netflix_Open_Content_Meridian/chapters/chapter_frames0000403-frames0000431.jpg\n",
    "\n",
    "#\n",
    "# notice that in get_contextual_information() above, we are going to compose the user and system prompt.  You can see the {0}\n",
    "# place holder in system_prompt above.\n",
    "#\n",
    "#  'system': system_prompt.format(user_prompt),\n",
    "#\n",
    "system_prompt, messages, contextual_response = prompt_llm(image_list, system_prompt, user_prompt)\n",
    "\n",
    "print(\"\\n\")\n",
    "display(\"user_prompt:\", user_prompt)\n",
    "print(\"\\n\")\n",
    "display(\"system_prompt:\", system_prompt)\n",
    "print(\"\\n\")\n",
    "display(\"messages\", JSON(remove_data_field(messages), expanded=1))\n",
    "print(\"\\n\")\n",
    "display(\"contextual_response:\", JSON(contextual_response, expanded=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9de17-2372-4a29-8a2e-6f498a98119e",
   "metadata": {},
   "source": [
    "# Summarize a Shot\n",
    "\n",
    "Let's ask Claude to summarize what's in a Shot.  Play around with the prompt to see how you get Claude to give you the information you want from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ee592-9bfd-4c7a-93cb-fffcac5bfdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_prompt = 'Summarize the scene'\n",
    "system_prompt = 'You are a media operation engineer. Your job is to review a portion of a video content presented in a sequence of consecutive images. Each image may also contain a sequence of frames presented in a 4x7 grid reading from left to right and then from top to bottom. {0} It is important to return the results in JSON format and also include a confidence score from 0 to 100. Skip any explanation.'\n",
    "\n",
    "\n",
    "idx = 7\n",
    "print(\"shot\", idx+1)\n",
    "shot = video['shots'].shots[idx]\n",
    "display(\"video['shots'] JSON ->\", JSON(shot))\n",
    "image_list = [ { 'file' :shot['composite_images'][0]['file'] } ]\n",
    "\n",
    "\n",
    "#\n",
    "# notice that in get_contextual_information() above, we are going to compose the user and system prompt.  You can see the {0}\n",
    "# place holder in system_prompt above.\n",
    "#\n",
    "#  'system': system_prompt.format(user_prompt),\n",
    "#\n",
    "system_prompt, messages, contextual_response = prompt_llm(image_list, system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2900c3-98be-4d1b-9bf2-99869af958c6",
   "metadata": {},
   "source": [
    "# Keep going\n",
    "\n",
    "Now go back up to where we first setup the user and system prompts.  Change the user_prompt and \"idx =\" line that selects a shot and see how the model output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5329197-72fa-41ea-a7ff-7948ca17e2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
