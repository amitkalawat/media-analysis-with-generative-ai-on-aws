{
 "cells": [
  {
   "cell_type": "raw",
   "id": "d4d800cb-f454-42c4-b19f-0c9be3dbb63e",
   "metadata": {},
   "source": [
    "# Image Prompt Playground\n",
    "\n",
    "In the Ad break detection and contextual Ad targeting notebook, we assembled video frames associated with topics and created composite image grids.  You can go back to that section if you'd like to review how and why we created the image grids for the LLM. \n",
    "\n",
    "Here, we are going to walk you through the prompt we generate using composite images and show you how the LLM is responding.\n",
    "\n",
    "First we will load all the necessary code and imports.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "882cead3-9d0c-4aea-be5d-a7f85ebb9960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import json\n",
    "import boto3\n",
    "import json_repair\n",
    "import copy\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "from lib import bedrock_helper as brh\n",
    "from lib import frame_utils\n",
    "from lib import util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7635e98-58aa-4a17-b27c-bc19683f8440",
   "metadata": {},
   "source": [
    "### Retrieve saved values from previous notebooks\n",
    "To run this notebook, you need to have run the previous notebook: 00_prerequisites.ipynb, where you installed package dependencies and gathered some information from the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d2d6ed2-1a7d-430c-9516-622c720a80c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd9a68db-4f70-4e75-8988-ed668cafc136",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 98444  100 98444    0     0   271k      0 --:--:-- --:--:-- --:--:--  271k\n"
     ]
    }
   ],
   "source": [
    "iab_file = 'iab_content_taxonomy_v3.json'\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/iab/{iab_file}\"\n",
    "\n",
    "!curl {url} -o {iab_file}\n",
    "#%% raw\n",
    "def load_iab_taxonomies(file):\n",
    "    with open(file) as f:\n",
    "        iab_taxonomies = json.load(f)\n",
    "    return iab_taxonomies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1713fd6-6634-43a2-a9ed-9ddae932ff94",
   "metadata": {},
   "source": [
    "# Generate contextual information from Claude\n",
    "\n",
    "Below you find the get_contextual_information() method that is in our lib/bedrock_helper.py python module.  We've pulled it out here so you can see the prompts and play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5b71bed5-f71e-4be9-be6e-2b06e830ce5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_contextual_information(images, text, iab_definitions):\n",
    "    task_all = 'You are asked to provide the following information: a detail description to describe the scene, identify the most relevant IAB taxonomy, GARM, sentiment, and brands and logos that may appear in the scene, and five most relevant tags from the scene.'\n",
    "    task_iab_only = 'You are asked to identify the most relevant IAB taxonomy.'\n",
    "    system = 'You are a media operation engineer. Your job is to review a portion of a video content presented in a sequence of consecutive images. Each image also contains a sequence of frames presented in a 4x7 grid reading from left to right and then from top to bottom. You may also optionally be given the conversation of the scene that helps you to understand the context. {0} It is important to return the results in JSON format and also includes a confidence score from 0 to 100. Skip any explanation.';\n",
    "\n",
    "    messages = []\n",
    " \n",
    "    # adding sequences of composite images to the prompt.  Limit is 20.\n",
    "    message_images = brh.make_image_message(images[:19])\n",
    "    messages.append(message_images)\n",
    "\n",
    "    # adding the conversation to the prompt\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the images. Do you have the conversation of the scene?'\n",
    "    })\n",
    "    message_conversation = brh.make_conversation_message(text)\n",
    "    messages.append(message_conversation)\n",
    "\n",
    "    # other information\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. Do you have other information to provdie?'\n",
    "    })\n",
    "\n",
    "    other_information = []\n",
    "    ## iab taxonomy\n",
    "    iab_list = brh.make_iab_taxonomoies(iab_definitions['tier1'])\n",
    "    other_information.append(iab_list)\n",
    "\n",
    "    ## GARM\n",
    "    garm_list = brh.make_garm_taxonomoies()\n",
    "    other_information.append(garm_list)\n",
    "\n",
    "    ## Sentiment\n",
    "    sentiment_list = brh.make_sentiments()\n",
    "    other_information.append(sentiment_list)\n",
    "\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': other_information\n",
    "    })\n",
    "\n",
    "    # output format\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. What output format?'\n",
    "    })\n",
    "    output_format = brh.make_output_example()\n",
    "    messages.append(output_format)\n",
    "\n",
    "    # prefill '{'\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })\n",
    "    \n",
    "    model_params = {\n",
    "        'anthropic_version': brh.MODEL_VER,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': system.format(task_all),\n",
    "        'messages': messages\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = brh.inference(model_params)\n",
    "    except Exception as e:\n",
    "        print(colored(f\"ERR: inference: {str(e)}\\n RETRY...\", 'red'))\n",
    "        response = inference(model_params)\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc2c534-5781-4e29-992a-5fc395caac06",
   "metadata": {},
   "source": [
    "# Generate contextual information from Claude\n",
    "\n",
    "Below you find the get_contextual_information() method that is in our lib/bedrock_helper.py python module.  We've pulled it out here so you can see the prompts and play around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "051afac4-0293-4f70-8eb0-01ec297bed95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Description: \u001b[32mThe scene depicts a city street with cars parked along the side and a multi-story building in the background. The building appears to be an office or commercial structure with a sign on the front.\u001b[0m (95%)\n",
      "Sentiment: \u001b[32mNeutral\u001b[0m (90%)\n",
      "Iab_taxonomy: \u001b[32mTravel\u001b[0m (80%)\n",
      "Garm_taxonomy: \u001b[32mNone\u001b[0m (95%)\n",
      "Brands_and_logos: \u001b[32mNone\u001b[0m\n",
      "Relevant_tags: \u001b[32mcity street, office building, parked cars, palm tree, urban scene\u001b[0m\n",
      "================================================\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "========================================================================\n",
      "Estimated cost: \u001b[32m$0.0071\u001b[0m in us-east-1 region with \u001b[32m972\u001b[0m input tokens and \u001b[32m279\u001b[0m output tokens.\n",
      "========================================================================\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "total_usage = {\n",
    "    'input_tokens': 0,\n",
    "    'output_tokens': 0,\n",
    "}\n",
    "\n",
    "iab_definitions = load_iab_taxonomies(iab_file)\n",
    "\n",
    "# for chapter in video['chapters'].chapters:\n",
    "\n",
    "\n",
    "\n",
    "# image_list = [\n",
    "#    {'file': './Netflix_Open_Content_Meridian/frames/frames0000019.jpg'},\n",
    "#    {'file': './Netflix_Open_Content_Meridian/frames/frames0000020.jpg'},\n",
    "#    {'file': './Netflix_Open_Content_Meridian/frames/frames0000021.jpg'}\n",
    "# ]\n",
    "\n",
    "image_list = [\n",
    "    {'file': './Netflix_Open_Content_Meridian/chapters/chapter_frames0000017-frames0000018.jpg'}\n",
    "]\n",
    "\n",
    "text = ''\n",
    "contextual_response = get_contextual_information(image_list, text, iab_definitions)\n",
    "time.sleep(5)\n",
    "usage = contextual_response['usage']\n",
    "contextual = contextual_response['content'][0]['json']\n",
    "\n",
    "# TOTO: commented out\n",
    "# save the contextual to the chapter\n",
    "# chapter['contextual'] = {\n",
    "#    'usage': usage,\n",
    "#    **contextual\n",
    "# }\n",
    "\n",
    "total_usage['input_tokens'] += usage['input_tokens']\n",
    "total_usage['output_tokens'] += usage['output_tokens']\n",
    "\n",
    "# print(f\"==== Contextual information ======\")\n",
    "#video['frames'].display_frames(start=chapter['start_frame_id'], end=chapter['end_frame_id']+1)\n",
    "for key in ['description', 'sentiment', 'iab_taxonomy', 'garm_taxonomy']:\n",
    "    print(f\"{key.capitalize()}: {colored(contextual[key]['text'], 'green')} ({contextual[key]['score']}%)\")\n",
    "\n",
    "for key in ['brands_and_logos', 'relevant_tags']:\n",
    "    items = ', '.join([item['text'] for item in contextual[key]])\n",
    "    if len(items) == 0:\n",
    "        items = 'None'\n",
    "    print(f\"{key.capitalize()}: {colored(items, 'green')}\")\n",
    "print(f\"================================================\\n\\n\")\n",
    "\n",
    "#output_file = os.path.join(video[\"output_dir\"], 'scenes_in_chapters.json')\n",
    "#util.save_to_file(output_file, video['chapters'].chapters)\n",
    "\n",
    "contextual_cost = brh.display_contextual_cost(total_usage)\n",
    "#%% raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662643da-4b7a-4586-9fc8-4ef2090d2dec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
