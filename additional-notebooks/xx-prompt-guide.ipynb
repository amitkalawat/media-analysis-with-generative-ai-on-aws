{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80217267-554b-477e-9348-265de77e6325",
   "metadata": {},
   "source": [
    "# Image Prompt Playground\n",
    "\n",
    "The [Prerequisites](00-prerequisites.ipynb), [Video segments: frames, shots and scenes](01A-visual-segments-frames-shots-scenes.ipynb) and [Ad Breaks and Contextual Ad Targeting](02-ad-breaks-and-contextual-ad-targeting.ipynb) notebooks are prerequisites for this prompting exercise.\n",
    "\n",
    "In the Ad break detection and contextual Ad targeting notebook, we assembled video frames associated with topics and created composite image grids.  You can go back to that notebook and look at the section titled \"Generate chapter level contextual information\" for a text and visual description of the flow.  \n",
    "\n",
    "Here, we are going to walk you through the prompt we generate using composite or single images (your choice) and show you how Claude responds based on the prompting.\n",
    "\n",
    "First let's load all the necessary code and imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab8d01a9-5a97-40f8-8202-5de30869a8e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import python packages\n",
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import boto3\n",
    "import json_repair\n",
    "import copy\n",
    "import time\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "from lib import bedrock_helper as brh\n",
    "from lib import frame_utils\n",
    "from lib import util\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "from io import BytesIO\n",
    "\n",
    "%store -r\n",
    "\n",
    "### Load the IAB Taxonomy\n",
    "iab_file = 'iab_content_taxonomy_v3.json'\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/iab/{iab_file}\"\n",
    "\n",
    "!curl {url} -o {iab_file}\n",
    "#%% raw\n",
    "def load_iab_taxonomies(file):\n",
    "    with open(file) as f:\n",
    "        iab_taxonomies = json.load(f)\n",
    "    return iab_taxonomies\n",
    "\n",
    "iab_definitions = load_iab_taxonomies(iab_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d137d5cd-4647-40f1-98c3-b804bad7a4d5",
   "metadata": {},
   "source": [
    "# Supporting code\n",
    "\n",
    "Below you will find a number of methods we pulled out of our [bedrock_helper](./lib/bedrock_helper.py) python module.  We've done this to expose the prompts so that you can see how your prompts impact the response.\n",
    "\n",
    "make_image_message() will take a number of images and base64 encode them so we can send them to Claude for understanding.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc37378-9034-4743-b23e-880bc3139a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_conversation_message(text):\n",
    "    message = {\n",
    "        'role': 'user',\n",
    "        'content': 'No conversation.'\n",
    "    }\n",
    "\n",
    "    if text:\n",
    "        message['content'] = 'Here is the conversation of the scene in <conversation> tag.\\n<conversation>\\n{0}\\n</conversation>\\n'.format(text)\n",
    "\n",
    "    return message\n",
    "    \n",
    "\n",
    "#\n",
    "# encode the images along with a prompt so that we can get some context about our frames or scenes from the LLM\n",
    "#\n",
    "def make_image_message(composite_images):\n",
    "    # adding the composite image sequences\n",
    "    image_contents = [{\n",
    "        'type': 'text',\n",
    "        'text': 'Here are {0} images containing a frame sequence that describes a scene.'.format(len(composite_images))\n",
    "    }]\n",
    "\n",
    "    open_images = []\n",
    "    for image in composite_images:\n",
    "        with open(image['file'], \"rb\") as image_file:\n",
    "            image_data = image_file.read()\n",
    "            open_images.append(image_file)\n",
    "        image_pil = Image.open(BytesIO(image_data))\n",
    "        bas64_image = frame_utils.image_to_base64(image_pil)\n",
    "        image_contents.append({\n",
    "            'type': 'image',\n",
    "            'source': {\n",
    "                'type': 'base64',\n",
    "                'media_type': 'image/jpeg',\n",
    "                'data': bas64_image\n",
    "            }\n",
    "        })\n",
    "\n",
    "    # close the images\n",
    "    for image in open_images:\n",
    "        image.close()\n",
    "\n",
    "    return {\n",
    "        'role': 'user',\n",
    "        'content': image_contents\n",
    "    }\n",
    "\n",
    "\n",
    "#\n",
    "# get rid of the encoded image data which results in a very long output message and just clutters things up\n",
    "#\n",
    "def remove_data_field(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: remove_data_field(v) if k != 'data' else '<snip ...>' for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [remove_data_field(item) for item in obj]\n",
    "    return obj\n",
    "\n",
    "#\n",
    "# use this method to show what's being passed to the LLM.  It will strip out the encoded image\n",
    "# information that clutters up the output. \n",
    "#\n",
    "def show_llm_messages(model_params):\n",
    "    print(f'\\nMessages:\\n')\n",
    "    for message in model_params:\n",
    "        try:\n",
    "            cleaned_message = remove_data_field(message)\n",
    "            json_string = json.dumps(cleaned_message, indent=2)\n",
    "            print(json_string)\n",
    "        except Exception as e:\n",
    "            print(f\"Error encoding message: {e}\")\n",
    "    print('\\n')\n",
    "\n",
    "\n",
    "#\n",
    "# we can remove this before release\n",
    "#\n",
    "def debug_message(messages):\n",
    "    for i, message in enumerate(messages):\n",
    "        print(f\"Message {i} type: {type(message)}\")\n",
    "        if isinstance(message, dict):\n",
    "            print(f\"Message {i} keys: {message.keys()}\")\n",
    "        elif isinstance(message, list):\n",
    "            print(f\"Message {i} length: {len(message)}\")\n",
    "            for j, item in enumerate(message):\n",
    "                print(f\"  Item {j} type: {type(item)}\")\n",
    "                if isinstance(item, dict):\n",
    "                    print(f\"  Item {j} keys: {item.keys()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425fca6d-3967-4a15-87fd-d5e94c8130dd",
   "metadata": {},
   "source": [
    "# Get Contextual Information\n",
    "\n",
    "Below you find the get_contextual_information() method that is in our [bedrock_helper](./lib/bedrock_helper.py) python module.  We've made some modifications so that you can pass a variety of system and user prompts and observe the different outputs from Claude. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ba308d-32d2-4ea7-81c9-a56f7c22fd49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# This is the meat of the code that builds up the conversation to pass to the LLM.\n",
    "#\n",
    "def get_contextual_information(images, conversation_text, system_prompt, user_prompt, iab_definitions):\n",
    "    task_iab_only = 'You are asked to identify the most relevant IAB taxonomy.'\n",
    "\n",
    "    messages = []\n",
    "    # adding sequences of composite images to the prompt.  Limit is 20.\n",
    "    message_images = make_image_message(images[:19])\n",
    "    messages.append(message_images)\n",
    "\n",
    "    # adding the conversation to the prompt\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'Got the images. Do you have the conversation of the scene?'\n",
    "    })\n",
    "    message_conversation = make_conversation_message(conversation_text)\n",
    "    messages.append(message_conversation)\n",
    "\n",
    "    # other information\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. Do you have other information to provdie?'\n",
    "    })\n",
    "\n",
    "    other_information = []\n",
    "    ## iab taxonomy\n",
    "    iab_list = brh.make_iab_taxonomoies(iab_definitions['tier1'])\n",
    "    other_information.append(iab_list)\n",
    "\n",
    "    ## GARM\n",
    "    garm_list = brh.make_garm_taxonomoies()\n",
    "    other_information.append(garm_list)\n",
    "\n",
    "    ## Sentiment\n",
    "    sentiment_list = brh.make_sentiments()\n",
    "    other_information.append(sentiment_list)\n",
    "\n",
    "    messages.append({\n",
    "        'role': 'user',\n",
    "        'content': other_information\n",
    "    })\n",
    "\n",
    "    # output format\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': 'OK. What output format?'\n",
    "    })\n",
    "    output_format = brh.make_output_example()\n",
    "    messages.append(output_format)\n",
    "\n",
    "    # prefill '{'\n",
    "    messages.append({\n",
    "        'role': 'assistant',\n",
    "        'content': '{'\n",
    "    })    \n",
    "    formatted_system_prompt = system_prompt.format(user_prompt)\n",
    "    model_params = {\n",
    "        'anthropic_version': brh.MODEL_VER,\n",
    "        'max_tokens': 4096,\n",
    "        'temperature': 0.1,\n",
    "        'top_p': 0.7,\n",
    "        'top_k': 20,\n",
    "        'stop_sequences': ['\\n\\nHuman:'],\n",
    "        'system': formatted_system_prompt,\n",
    "        'messages': messages\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = brh.inference(model_params)\n",
    "    except Exception as e:\n",
    "        print(colored(f\"ERR: inference: {str(e)}\\n RETRY...\", 'red'))\n",
    "        response = inference(model_params)\n",
    "\n",
    "    return formatted_system_prompt, messages, response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73ea79e-8c0d-4846-9783-0c441e06c348",
   "metadata": {},
   "source": [
    "# Code to Call Claude\n",
    "\n",
    "Below is the code we will use to display the image we are passing to Claude, call get_contextual_information() to have the actual conversation with the model and then display the cost of the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "698403eb-0715-4304-a37e-715a70288d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prompt_llm(image_list, conversation_text, system_prompt, user_prompt):\n",
    "\n",
    "    total_usage = {\n",
    "        'input_tokens': 0,\n",
    "        'output_tokens': 0,\n",
    "    }\n",
    "    \n",
    "    for idx, composite_image in enumerate(image_list):\n",
    "        print (f'\\nImage {idx+1 } of { len(image_list) }: { composite_image[\"file\"] }\\n')\n",
    "        display(DisplayImage(filename=composite_image['file']))\n",
    "    \n",
    "    conversation_text = ''\n",
    "    \n",
    "    print(\"\\nSending conversation to LLM ...\")\n",
    "    system_prompt, messages, contextual_response = get_contextual_information(image_list, conversation_text,\n",
    "                                                               system_prompt, user_prompt, iab_definitions)\n",
    "    print(\"Got a response\\n\")\n",
    "    \n",
    "    usage = contextual_response['usage']\n",
    "    contextual = contextual_response['content'][0]['json']\n",
    "    \n",
    "    total_usage['input_tokens'] += usage['input_tokens']\n",
    "    total_usage['output_tokens'] += usage['output_tokens']\n",
    "    \n",
    "    for key in ['description', 'sentiment', 'iab_taxonomy', 'garm_taxonomy']:\n",
    "        print(f\"{key.capitalize()}: {colored(contextual[key]['text'], 'green')} ({contextual[key]['score']}%)\")\n",
    "    \n",
    "    for key in ['brands_and_logos', 'relevant_tags']:\n",
    "        items = ', '.join([item['text'] for item in contextual[key]])\n",
    "        if len(items) == 0:\n",
    "            items = 'None'\n",
    "        print(f\"{key.capitalize()}: {colored(items, 'green')}\")\n",
    "    print(f\"================================================\")\n",
    "    \n",
    "    contextual_cost = brh.display_contextual_cost(total_usage)\n",
    "\n",
    "    return system_prompt, messages, contextual_response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0cfa43d-3553-45a3-9536-58e34f613800",
   "metadata": {},
   "source": [
    "# Let's do it!\n",
    "\n",
    "Below we define our system prompt as well as an image list for which we want to get contextual information.  You can find images in the Netflix_Open_Content_Meridian folder.  There are chapters, composite images, frames, scenes and shots to play with.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "949842b5-e15c-4966-8138-acdcac853a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_text = ''\n",
    "user_prompt = 'You are asked to provide the following information: a detail description to describe the scene, identify the most relevant IAB taxonomy, GARM, sentiment, and brands and logos that may appear in the scene, and five most relevant tags from the scene.'\n",
    "system_prompt = 'You are a media operation engineer. Your job is to review a portion of a video content presented in a sequence of consecutive images. Each image may also contain a sequence of frames presented in a 4x7 grid reading from left to right and then from top to bottom. You may also optionally be given the conversation of the scene that helps you to understand the context. {0} It is important to return the results in JSON format and also includes a confidence score from 0 to 100. Skip any explanation.'\n",
    "\n",
    "image_list = [\n",
    "    {'file': './Netflix_Open_Content_Meridian/chapters/chapter_frames0000053-frames0000081.jpg'}\n",
    " ]\n",
    "    \n",
    "#\n",
    "# notice that in get_contextual_information() above, we are going to compose the user and system prompt.  You can see the {0}\n",
    "# place holder in system_prompt above.\n",
    "#\n",
    "#  'system': system_prompt.format(user_prompt),\n",
    "#\n",
    "system_prompt, messages, contextual_response = prompt_llm(image_list, conversation_text, system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f9f40e-ff3f-4785-ba9b-f7df345175b0",
   "metadata": {},
   "source": [
    "# Conversation with Claude using Bedrock\n",
    "\n",
    "Let's take a look at the system prompt and messages we sent to Claude, we are using the [Anthropic Claude Messages API](https://docs.aws.amazon.com/bedrock/latest/userguide/model-parameters-anthropic-claude-messages.html).\n",
    "\n",
    "We've stripped out the base64 encoded image(s) for display purposes and replaced them with \"\\<snip\\>\".   The \"user\" messages are from the user (you) and the \"assistant\" is Claude.  In the messages, we add a prompt to see if we have conversational text around the scene, which we don't provide.  We are asked if we have anything else and we provide the IABM and GARM taxonomies for classification as well as sentiment tags.  The assistant asks us which format we want it in and we request JSON.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddcb908d-4f64-4a78-a90f-b6fc0ed2db23",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\")\n",
    "display(\"system_prompt:\", system_prompt)\n",
    "print(\"\\n\")\n",
    "display(\"messages\", JSON(remove_data_field(messages), expanded=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78ffacdd-6b15-41b8-a119-95c16bb315ec",
   "metadata": {},
   "source": [
    "# Changing a prompt\n",
    "\n",
    "Let's try changing the prompt and see what we get.  This time we will ask just about the images on the wall and let Claude know we don't want any more information than that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3df2413-60ec-40bb-bcc8-aeaabe5bff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_text = ''\n",
    "user_prompt = 'I would like you to tell me if there are any images on the wall and if so please describe them.  Take your time and look closely at the images. I only want to know about the images on the wall, nothing else.'\n",
    "system_prompt = 'You are a media operation engineer. Your job is to review a portion of a video content presented in a sequence of consecutive images. Each image may also contain a sequence of frames presented in a 4x7 grid reading from left to right and then from top to bottom. You may also optionally be given the conversation of the scene that helps you to understand the context. {0} It is important to return the results in JSON format and also includes a confidence score from 0 to 100. Skip any explanation.';\n",
    "\n",
    "image_list = [\n",
    "    {'file': './Netflix_Open_Content_Meridian/chapters/chapter_frames0000053-frames0000081.jpg'}\n",
    " ]\n",
    "\n",
    "# media-analysis-with-generative-ai-on-aws/Netflix_Open_Content_Meridian/chapters/chapter_frames0000403-frames0000431.jpg\n",
    "\n",
    "#\n",
    "# notice that in get_contextual_information() above, we are going to compose the user and system prompt.  You can see the {0}\n",
    "# place holder in system_prompt above.\n",
    "#\n",
    "#  'system': system_prompt.format(user_prompt),\n",
    "#\n",
    "system_prompt, messages, contextual_response = prompt_llm(image_list, conversation_text, system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82c9de17-2372-4a29-8a2e-6f498a98119e",
   "metadata": {},
   "source": [
    "# Summarize a Chapter\n",
    "\n",
    "Let's ask Claude to summarize what's in a chapter.  Play around with the prompt to see how you get Claude to give you the information you want from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d50ee592-9bfd-4c7a-93cb-fffcac5bfdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation_text = ''\n",
    "user_prompt = 'the following images are the scene changes for a chapter in the movie, give a narrative as to what you think is happening in the chapter.  Give as much detail about the person and landscape.  I would also like you to include what you think the weather is like.'\n",
    "system_prompt = 'You are a media operation engineer. Your job is to review a portion of a video content presented in a sequence of consecutive images. Each image may also contain a sequence of frames presented in a 4x7 grid reading from left to right and then from top to bottom. You may also optionally be given the conversation of the scene that helps you to understand the context. {0} It is important to return the results in JSON format and also includes a confidence score from 0 to 100. Skip any explanation.';\n",
    "\n",
    "image_list = [\n",
    "    {'file': './Netflix_Open_Content_Meridian/chapters/chapter_frames0000403-frames0000431.jpg'}\n",
    " ]\n",
    "\n",
    "\n",
    "#\n",
    "# notice that in get_contextual_information() above, we are going to compose the user and system prompt.  You can see the {0}\n",
    "# place holder in system_prompt above.\n",
    "#\n",
    "#  'system': system_prompt.format(user_prompt),\n",
    "#\n",
    "system_prompt, messages, contextual_response = prompt_llm(image_list, conversation_text, system_prompt, user_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2900c3-98be-4d1b-9bf2-99869af958c6",
   "metadata": {},
   "source": [
    "# Keep going\n",
    "\n",
    "You can continue to change the prompt and images that you pass to Claude and see how the model output changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5329197-72fa-41ea-a7ff-7948ca17e2ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
