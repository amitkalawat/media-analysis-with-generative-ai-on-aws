{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a8458ac4-23ed-461e-bf44-55432f709d1a",
   "metadata": {},
   "source": [
    "# Semantic Video Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c286d5-2bf1-4aff-99ca-74b9d929c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import uuid\n",
    "import time\n",
    "import logging\n",
    "import re\n",
    "import io\n",
    "from IPython.display import JSON\n",
    "import subprocess\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import glob\n",
    "import base64\n",
    "import math\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13068f49-0a48-4d1a-ac1c-c504dc0a58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f749e7-42c4-4c40-8079-6e180e32c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53cf6e98-471d-42d0-ad9a-36cc96fda229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def start_transcription_job(\n",
    "    job_name,\n",
    "    media_uri,\n",
    "    media_format,\n",
    "    language_code,\n",
    "    transcribe_client,\n",
    "    output_bucket_name,\n",
    "    vocabulary_name=None,\n",
    "):\n",
    "    \"\"\"\n",
    "    Starts a transcription job. This function returns as soon as the job is started.\n",
    "    To get the current status of the job, call get_transcription_job. The job is\n",
    "    successfully completed when the job status is 'COMPLETED'.\n",
    "\n",
    "    :param job_name: The name of the transcription job. This must be unique for\n",
    "                     your AWS account.\n",
    "    :param media_uri: The URI where the audio file is stored. This is typically\n",
    "                      in an Amazon S3 bucket.\n",
    "    :param media_format: The format of the audio file. For example, mp3 or wav.\n",
    "    :param language_code: The language code of the audio file.\n",
    "                          For example, en-US or ja-JP\n",
    "    :param transcribe_client: The Boto3 Transcribe client.\n",
    "    :param vocabulary_name: The name of a custom vocabulary to use when transcribing\n",
    "                            the audio file.\n",
    "    :return: Data about the job.\n",
    "    \"\"\"\n",
    "    job_args = {\n",
    "        \"TranscriptionJobName\": job_name,\n",
    "        \"Media\": {\"MediaFileUri\": media_uri},\n",
    "        \"MediaFormat\": media_format,\n",
    "        \"LanguageCode\": language_code,\n",
    "        \"Subtitles\": {\"Formats\": [\"srt\"]},\n",
    "        \"OutputBucketName\": output_bucket_name,\n",
    "    }\n",
    "    if vocabulary_name is not None:\n",
    "        job_args[\"Settings\"] = {\"VocabularyName\": vocabulary_name}\n",
    "    response = transcribe_client.start_transcription_job(**job_args)\n",
    "    job = response[\"TranscriptionJob\"]\n",
    "    return job\n",
    "\n",
    "def wait_for_transcription_complete(transcribe_client, job_id):\n",
    "    response = transcribe_client.get_transcription_job(\n",
    "        TranscriptionJobName=job_id\n",
    "    )\n",
    "    while True:\n",
    "        job_status = response['TranscriptionJob']['TranscriptionJobStatus']\n",
    "        print(f\"job_status: {job_status}\")\n",
    "        if job_status in ('FAILED', 'COMPLETED'):\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        response = transcribe_client.get_transcription_job(\n",
    "            TranscriptionJobName=job_id\n",
    "        )\n",
    "\n",
    "def start_rekognition_segment_detection(rek_client,\n",
    "                                        bucket_videos, \n",
    "                                        video_name, \n",
    "                                        min_Shot_Confidence=80.0):\n",
    "\n",
    "    response = rek_client.start_segment_detection(\n",
    "        Video={\"S3Object\": {\"Bucket\": bucket_videos, \"Name\": video_name}},\n",
    "        SegmentTypes=[\"SHOT\"],\n",
    "        Filters={\n",
    "            \"ShotFilter\": {\"MinSegmentConfidence\": min_Shot_Confidence},\n",
    "        },\n",
    "    )\n",
    "\n",
    "    startJobId = response[\"JobId\"]\n",
    "    return startJobId\n",
    "\n",
    "def wait_for_rekognition_segment_detection_complete(rek_client, job_id):\n",
    "    response = rek_client.get_segment_detection(JobId=job_id)\n",
    "    while True:\n",
    "        job_status = response[\"JobStatus\"]\n",
    "        print(f\"job_status: {job_status}\")\n",
    "        if job_status in ['SUCCEEDED', 'FAILED']:\n",
    "            break\n",
    "        time.sleep(5)\n",
    "        response = rek_client.get_segment_detection(JobId=job_id)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29e271c-b81d-4852-b4e5-b5a938f77a58",
   "metadata": {},
   "source": [
    "First we want to download some video assets and process them. These assets are stored in an S3 bucket publically for the workshop.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb758a09-85b2-4768-b1ff-a897ab44a403",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_name = \"Netflix_Open_Content_Meridian_5m.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80befc8f-025c-4c49-9127-ff6441687984",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ffmpeg -ss 00:0:30 -to 00:5:30 -i Netflix_Open_Content_Meridian.mp4 -c copy $video_name -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c79d29e-8e42-42c8-8077-6512458ae836",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget --no-check-certificate https://ws-assets-prod-iad-r-pdx-f3b3f9f1a7d6a3d0.s3.us-west-2.amazonaws.com/7db2455e-0fa6-4f6d-9973-84daccd6421f/$video_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "736ef11a-9f4d-4175-80e6-133f1522e3ba",
   "metadata": {},
   "source": [
    "Upload the assets to an S3 bucket in the same AWS account so that we could invoke API to process them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cde75070-5103-4f8d-87cf-b7fe5bc489f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_videos = session[\"bucket\"]\n",
    "bucket_transcripts = session[\"bucket\"]\n",
    "bucket_images = session[\"bucket\"]\n",
    "bucket_shots = session[\"bucket\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e39163ac-df9d-4daf-9d69-ed5335f40709",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp $video_name s3://$bucket_videos/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c484150d-c88d-4cc5-9be7-97ab51380de6",
   "metadata": {},
   "source": [
    "First, start a transcription job to extract the audio text from the video"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5c809e-f0a6-4cfb-ab64-7498adb1495c",
   "metadata": {},
   "source": [
    "## Step 1: Start a Transcription Job\n",
    "We are going to use [Amazon Transcribe](https://docs.aws.amazon.com/transcribe/latest/dg/what-is.html) to extracrt the audio text from the video. Amazon Transcribe is an automatic speech recognition service that uses machine learning models to convert audio to text. You can use Amazon Transcribe as a standalone transcription service or to add speech-to-text capabilities to any application.\n",
    "\n",
    "With Amazon Transcribe, you can improve accuracy for your specific use case with language customization, filter content to ensure customer privacy or audience-appropriate language, analyze content in multi-channel audio, partition the speech of individual speakers, and more.\n",
    "\n",
    "You can transcribe media in real time (streaming) or you can transcribe media files located in an Amazon S3 bucket (batch). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f57b79-5a65-4fe0-bc7c-d6c3f0882e2b",
   "metadata": {},
   "source": [
    "Initialze AWS SDK clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a198390-1501-4672-b98a-cd240a4c91bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_client = boto3.client(\"transcribe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f977cf-6219-4fc2-88c5-a419fd197332",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_jobs = []\n",
    "jobId = uuid.uuid4().hex # this could be a unique identifier that you can pass throughout the session \n",
    "job = start_transcription_job(\n",
    "        jobId,\n",
    "        \"s3://\" + bucket_videos + \"/\" + video_name,\n",
    "        \"mp4\",\n",
    "        \"en-US\",\n",
    "        transcribe_client,\n",
    "        bucket_transcripts,\n",
    "        None,\n",
    "    )\n",
    "transcribe_jobs.append(jobId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6034500-b48d-45f5-95b2-b794ac4037ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_for_transcription_complete(transcribe_client, jobId)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "098fa473-f50f-4594-b3da-271f9443f537",
   "metadata": {},
   "source": [
    "## Step 2: Detect Video Segments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2307623a-7f35-4579-ae9b-78adbd22867e",
   "metadata": {},
   "source": [
    "Initialze AWS SDK clients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fe4dff4-2e08-4bd5-b76c-ac88cdd51679",
   "metadata": {},
   "outputs": [],
   "source": [
    "rek_client = boto3.client(\"rekognition\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267e093-6ec2-4b15-a08c-39104e640f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_Shot_Confidence = 90.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bd011e-9a9e-4a2b-b7b1-87585959c28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rek_job_id = start_rekognition_segment_detection(rek_client,\n",
    "                                    bucket_videos, \n",
    "                                    video_name, \n",
    "                                    min_Shot_Confidence=min_Shot_Confidence)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22a6b7a2-9056-414c-9231-b410d517d868",
   "metadata": {},
   "source": [
    "Wait for the video sgement detection job to complete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e47074d-e2e4-4f35-bd1c-da20fd0d19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "rek_shot_segment_response = wait_for_rekognition_segment_detection_complete(rek_client, rek_job_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82b35447-9e31-4a62-8b98-a4b73b81974e",
   "metadata": {},
   "source": [
    "Processing the shot segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44124583-8cd1-428d-9767-bb479646a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_frames(shot, N): \n",
    "        '''\n",
    "        Extracts N frames from the given shot. This step is performed to reduce the compute required to process a shot.\n",
    "        Input: shot - shot object with images,\n",
    "               N - number of images to extract from the shot\n",
    "        Returns: extracted frames\n",
    "        '''\n",
    "        start_frame = shot[\"StartFrameNumber\"]\n",
    "        end_frame = (\n",
    "            shot[\"EndFrameNumber\"] - 1\n",
    "        )  # frame - 1 to avoid bug not getting the last frame of the video\n",
    "        step = (end_frame - start_frame) / (N - 1)\n",
    "        frames = [int(start_frame + i * step) for i in range(N)]\n",
    "        return frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a931e22a-c9cb-4b11-b3ec-1ef37c9667fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "shots  = []\n",
    "delta = rek_shot_segment_response[\"Segments\"][0][\"StartTimestampMillis\"]\n",
    "frames_per_shot = 4 # number of frames to extract from a detected shot.\n",
    "for shot in rek_shot_segment_response[\"Segments\"]:\n",
    "    shot_frames = get_frames(shot, frames_per_shot)\n",
    "    frames.extend(shot_frames)\n",
    "    # frames.append(shot[\"StartFrameNumber\"])\n",
    "\n",
    "    shot_startTime = shot[\"StartTimestampMillis\"] - delta\n",
    "    shot_endTime = shot[\"EndTimestampMillis\"] - delta\n",
    "\n",
    "    shots.append(\n",
    "        {\n",
    "            \"jobId\": jobId,\n",
    "            \"video_name\": video_name,\n",
    "            \"shot_startTime\": shot_startTime,\n",
    "            \"shot_endTime\": shot_endTime,\n",
    "            \"frames\": shot_frames,\n",
    "        }\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2daf576-b704-41d7-9283-b3a019aa6574",
   "metadata": {},
   "outputs": [],
   "source": [
    "rek_shot_segment_response[\"Segments\"][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4355c378-41f5-4577-a6bc-37117025a845",
   "metadata": {},
   "source": [
    "Let's visualize the first 10 shots in JSON structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9651b53c-d753-4c48-a0f9-493d8c65b7e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(shots[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ff3831-d7df-4060-b5d8-b2fc2fbc09ca",
   "metadata": {},
   "source": [
    "### Step 2.1: Generate Shot Images\n",
    "Extract images based on the given frame indexes, then upload the frames to the given S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72ba340d-5a4c-40bb-8f38-4e83b412afd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "tmp_dir = \"/tmp\"\n",
    "tmp_video_dir = tmp_dir + \"/video/\"\n",
    "tmp_frames_dir = tmp_dir + \"/\" + jobId + \"/\"\n",
    "os.makedirs(tmp_video_dir, exist_ok=True)\n",
    "os.makedirs(tmp_frames_dir, exist_ok=True)\n",
    "ffmpeg_path = \"/usr/bin/ffmpeg\"\n",
    "local_video_path = os.path.join(tmp_video_dir, video_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b74911a-77ce-4e8b-b1d8-fe00e362a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client.download_file(bucket_videos, video_name, local_video_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e10572-4ce0-4db2-9929-371a074a5963",
   "metadata": {},
   "source": [
    "verify that the files are downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73833716-5c10-49b2-b970-92b041a15ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -lrt $local_video_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f27d05b1-05f4-4900-ade6-e58b0a9941e8",
   "metadata": {},
   "source": [
    "Given the frame level information, in the following section, we are going to use [ffmpeg](https://www.ffmpeg.org/) to help us extract the corresponding images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95bc71ae-df13-4b27-94c0-8eff0a6df520",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_list = \"+\".join([f\"eq(n,{frame})\" for frame in frames])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce500a0e-6fe7-4746-ad5c-660dffc5e672",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_pattern = f\"{tmp_frames_dir}%d.png\"\n",
    "_ = subprocess.run(\n",
    "    [\n",
    "        ffmpeg_path,\n",
    "        \"-i\",\n",
    "        local_video_path,\n",
    "        \"-vf\",\n",
    "        f\"select='{frame_list}'\",\n",
    "        \"-vsync\",\n",
    "        \"0\",\n",
    "        \"-frame_pts\",\n",
    "        \"1\",\n",
    "        output_pattern,\n",
    "    ],\n",
    "    stderr=subprocess.PIPE\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8128996-7c7b-431b-b593-280b590d1f1d",
   "metadata": {},
   "source": [
    "Let's visualize the first 5 shots extracted from the shot segment job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b392b47-0789-47a7-a1be-55c2c5cb6a0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = glob.glob(f\"{tmp_frames_dir}/*.png\")\n",
    "\n",
    "for shot_nbr in range(5):\n",
    "    fig, axes = plt.subplots(1, frames_per_shot, figsize=(20, 3))\n",
    "    fig.suptitle(f\"Shot #{shot_nbr+1} extracted from Rekognition Shot Segment Detection job\", fontsize=14)\n",
    "    for i, ax in enumerate(axes.flat):\n",
    "        img = Image.open(image_paths[i+(shot_nbr*frames_per_shot)])\n",
    "        ax.imshow(img)\n",
    "        ax.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6a6078-685f-44d6-b83e-d8bf40271c24",
   "metadata": {},
   "source": [
    "Upload the image frames to S3 bucket for further processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de6a169-3659-428b-8995-7c23c20c442e",
   "metadata": {},
   "outputs": [],
   "source": [
    "extra_args = {\"ContentType\": \"image/png\"}\n",
    "for frame_file in os.listdir(tmp_frames_dir):\n",
    "    frame_path = os.path.join(tmp_frames_dir, frame_file)\n",
    "    s3_client.upload_file(\n",
    "        frame_path, bucket_images, f\"{jobId}/images/{frame_file}\", ExtraArgs=extra_args\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f044262-9499-4ab4-bdec-2ac06111953b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_shot_image(images, bucket_shots, shot_id, border_size=5):\n",
    "    num_images = len(images)\n",
    "    grid_size = math.ceil(math.sqrt(num_images))\n",
    "    grid_width = grid_size * images[0].width\n",
    "    grid_height = grid_size * images[0].height\n",
    "    grid_image = Image.new(\"RGB\", (grid_width, grid_height))\n",
    "\n",
    "    ## Horizontal grid ###\n",
    "    grid_width = sum(image.width + border_size for image in images) - border_size\n",
    "    grid_height = max(image.height for image in images)\n",
    "    grid_image = Image.new(\"RGB\", (grid_width, grid_height))\n",
    "    x_offset = 0\n",
    "    for image in images:\n",
    "        grid_image.paste(image, (x_offset, 0))\n",
    "        x_offset += image.width + border_size\n",
    "\n",
    "    with io.BytesIO() as buffer:\n",
    "        grid_image.save(buffer, format=\"PNG\")\n",
    "        buffer.seek(0)\n",
    "        image_base64 = base64.b64encode(buffer.getvalue()).decode(\"utf-8\")\n",
    "        s3_client.upload_fileobj(\n",
    "            buffer,\n",
    "            bucket_shots,\n",
    "            f\"{jobId}/shots/{shot_id}.png\",\n",
    "            ExtraArgs={\"ContentType\": \"image/png\"},\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36891e08-93f5-4bf3-a0a4-d8d34efeac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e88e3e0-76fb-4da6-88c8-5e3849c6bb77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "def process_shot(shot):\n",
    "    frames = shot['frames']\n",
    "    shot_startTime = shot['shot_startTime']\n",
    "    shot_endTime = shot['shot_endTime']\n",
    "    jobId = shot['jobId']\n",
    "    images = []\n",
    "    shot_id = f\"{shot_startTime}-{shot_endTime}\"\n",
    "    for frame in frames:\n",
    "        frame_path = os.path.join(tmp_frames_dir, f\"{frame}.png\")\n",
    "        images.append(Image.open(frame_path))\n",
    "    generate_shot_image(images, bucket_shots, shot_id)\n",
    "    return shot_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5f3869-10dd-459e-addf-4e6fb9b57784",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_processing_shots = [ (x, ) for x in shots ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c4659db-97e9-49f2-a234-b6b32daa6a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    shot_ids = pool.starmap(process_shot, multi_processing_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cad747-5f9a-42c7-9bd5-853464292254",
   "metadata": {},
   "source": [
    "Let's visualize a few shot images generated in the previous step to give us an idea of what they look like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2294be13-1ff7-4a82-82b7-81b8106174ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_id_sample = shot_ids[0]\n",
    "shot_id_sample_image = s3_client.get_object(Bucket=bucket_images, Key=f\"{jobId}/shots/{shot_id_sample}.png\")\n",
    "image_data = shot_id_sample_image[\"Body\"].read()\n",
    "image = Image.open(io.BytesIO(image_data))\n",
    "\n",
    "# Display the image\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351d55a-0263-4500-9d0a-f9314fbcb4c2",
   "metadata": {},
   "source": [
    "### Step 3: Celebrity Detection\n",
    "In addition to segment detection above, [Amazon Rekognition](https://aws.amazon.com/rekognition/) can also be used to recognize international, widely known celebrities like actors, sportspeople, and online content creators. The metadata provided by the celebrity recognition API significantly reduces the repetitive manual effort required to tag content and make it readily searchable. \n",
    "In the following section, we'll leverage this feature to help us detect any celebrities in the images extracted in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a748318-a993-4685-b8fe-6b0c2b3fa770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def celebrity_detection(shot):\n",
    "    \"\"\"\n",
    "    Invokes Amazon Rekognition API for celebrity Detection.\n",
    "    Filter the celebrities by the confident score. \n",
    "    Input: shot level information\n",
    "    Output: detected celebrity and associated objects\n",
    "    \"\"\"\n",
    "    shot_frames_public_figures = []\n",
    "    frames = shot[\"frames\"]\n",
    "    for frame in frames:\n",
    "        object_key = f\"{jobId}/images/{frame}.png\"\n",
    "        response = rek_client.recognize_celebrities(\n",
    "            Image={\n",
    "                \"S3Object\": {\"Bucket\": bucket_images, \"Name\": object_key}\n",
    "            }\n",
    "        )\n",
    "        celebrities = set()\n",
    "        min_confidence = 80.0 # change this value if the accuracy is low.\n",
    "\n",
    "        for celebrity in response.get(\"CelebrityFaces\", []):\n",
    "            if celebrity.get(\"MatchConfidence\", 0.0) >= min_confidence:\n",
    "                celebrities.add(celebrity[\"Name\"])\n",
    "        celebrities = \", \".join(celebrities)\n",
    "        shot_frames_public_figures.append({\"frame\": frame, \"frame_publicFigures\": celebrities})\n",
    "    return shot_frames_public_figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1b92946-914c-4db8-ab08-84989a737b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_processing_shots = [ (x, ) for x in shots ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e53a7d-6c3c-4bc5-9288-2eda9f766eff",
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    shot_frames_public_figures = pool.starmap(celebrity_detection, multi_processing_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b288ca9-0012-4655-871c-bac27594c322",
   "metadata": {},
   "source": [
    "Let's take a look into the celebrity detection from one of the frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223480a5-cca5-46f6-b144-e985b268d59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(shot_frames_public_figures)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb7a43f-d402-41b8-91fe-c1887adcbe06",
   "metadata": {},
   "source": [
    "<!-- In addition to using the rekognition API for public celebrity detection, we can also use a Foundation Model to help identify those who are private figures. In the example below, we are using an Anthropic Claude 3 Sonnet model to help us provide those insights by feeding the image frames and a prompt. -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3257c5c-eb42-4950-8470-21c47a68b42e",
   "metadata": {},
   "source": [
    "### Step 4: Data Ingestion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e178fdb-1c75-491d-a1ab-e472019e2909",
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\")\n",
    "embedding_model = \"amazon.titan-embed-image-v1\"\n",
    "embedding_model_output_dim = 1024 # this is specific to the titan_embed_image_v1 model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c26eb1c-80fd-41fd-9d07-3a236622e2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titan_image_embedding(embedding_model, image_name):\n",
    "    frame_path = os.path.join(tmp_frames_dir, image_name)\n",
    "    with open(frame_path, \"rb\") as image_file:\n",
    "        base64_image_string = base64.b64encode(image_file.read()).decode('utf8')\n",
    "\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    body = json.dumps(\n",
    "        {\"inputImage\": base64_image_string}\n",
    "    )\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body, modelId=embedding_model, accept=accept, contentType=content_type\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d4371-bd0f-4cf3-bb8c-fc8009ef4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_opensearch_client(host, region):\n",
    "    host = host.split(\"://\")[1] if \"://\" in host else host\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    auth = AWSV4SignerAuth(credentials, region, \"aoss\")\n",
    "\n",
    "    client = OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        pool_maxsize=20,\n",
    "    )\n",
    "\n",
    "    return client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab06adc7-67a3-4f8c-96c2-845fa48c1700",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_opensearch_index(host, region, index, len_embedding):\n",
    "    host = host.split(\"://\")[1] if \"://\" in host else host\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    auth = AWSV4SignerAuth(credentials, region, \"aoss\")\n",
    "\n",
    "    client = OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        pool_maxsize=20,\n",
    "    )\n",
    "\n",
    "    exist = client.indices.exists(index)\n",
    "    if not exist:\n",
    "        print(\"Creating index\")\n",
    "        index_body = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"jobId\": {\"type\": \"text\"},\n",
    "                    \"video_name\": {\"type\": \"text\"},\n",
    "                    \"shot_id\": {\"type\": \"text\"},\n",
    "                    \"shot_startTime\": {\"type\": \"text\"},\n",
    "                    \"shot_endTime\": {\"type\": \"text\"},\n",
    "                    \"shot_description\": {\"type\": \"text\"},\n",
    "                    \"shot_publicFigures\": {\"type\": \"text\"},\n",
    "                    \"shot_transcript\": {\"type\": \"text\"},\n",
    "                    \"shot_image_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                    \"shot_desc_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"number_of_shards\": 2,\n",
    "                    \"knn.algo_param\": {\"ef_search\": 512},\n",
    "                    \"knn\": True,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        response = client.indices.create(index, body=index_body)\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89164edc-85df-468f-94f9-e1e529c7a23d",
   "metadata": {},
   "outputs": [],
   "source": [
    "open_search_index = \"video_semantic_search_index\"\n",
    "host = session[\"AOSSCollectionEndpoint\"]\n",
    "region = sagemaker_resources[\"region\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b0d51e-bc87-4ab0-881b-057074a35d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_client = create_opensearch_index(host, region, open_search_index, embedding_model_output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb85b16f-a381-4193-8a8d-0e6ddfed392d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shot_collection(host, region, index, len_embedding):\n",
    "    host = host.split(\"://\")[1] if \"://\" in host else host\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    auth = AWSV4SignerAuth(credentials, region, \"aoss\")\n",
    "\n",
    "    client = OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        pool_maxsize=20,\n",
    "    )\n",
    "\n",
    "    exist = client.indices.exists(index)\n",
    "    if not exist:\n",
    "        print(\"Creating index\")\n",
    "        index_body = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"jobId\": {\"type\": \"text\"},\n",
    "                    \"video_name\": {\"type\": \"text\"},\n",
    "                    \"shot_id\": {\"type\": \"text\"},\n",
    "                    \"shot_startTime\": {\"type\": \"text\"},\n",
    "                    \"shot_endTime\": {\"type\": \"text\"},\n",
    "                    \"frame_publicFigures\": {\"type\": \"text\"},\n",
    "                    \"frame_image_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"faiss\",\n",
    "                            \"space_type\": \"l2\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"number_of_shards\": 2,\n",
    "                    \"knn.algo_param\": {\"ef_search\": 512},\n",
    "                    \"knn\": True,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        response = client.indices.create(index, body=index_body)\n",
    "\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c87211-728b-4004-bf3e-e7927211d27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "opensearch_client = create_shot_collection(host, region, jobId, embedding_model_output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd2a2048-8290-443e-9ce3-cdf449107bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embedding(shot, shot_frames_public_figures, index):\n",
    "    frames = shot[\"frames\"]\n",
    "    jobId = shot[\"jobId\"]\n",
    "    video_name = shot[\"video_name\"]\n",
    "    shot_start_time = shot[\"shot_startTime\"]\n",
    "    shot_endTime = shot[\"shot_endTime\"]\n",
    "    for idx, frame in enumerate(frames):\n",
    "        embedding = get_titan_image_embedding(\n",
    "            embedding_model, f\"{frame}.png\"\n",
    "        )\n",
    "        embedding_request_body = json.dumps(\n",
    "            {\n",
    "                \"jobId\": jobId,\n",
    "                \"video_name\": video_name,\n",
    "                \"shot_startTime\": shot_startTime,\n",
    "                \"shot_endTime\": shot_endTime,\n",
    "                \"frame_publicFigures\": shot_frames_public_figures[idx][\"frame_publicFigures\"],\n",
    "                \"frame_image_vector\": embedding,\n",
    "            }\n",
    "        )\n",
    "        response = opensearch_client.index(\n",
    "            index=index,\n",
    "            body=embedding_request_body,\n",
    "            params={\"timeout\": 60},\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45c0439-977c-4861-8eca-25698b656153",
   "metadata": {},
   "source": [
    "Create an input list for all the shots. The list is to be used in the multiprocessing job to allow the shot level indexing jobs to run in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb606af0-95cf-413f-9adf-eb68a1f6afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = []\n",
    "for idx in range(len(shots)):\n",
    "  inputs.append((shots[idx], shot_frames_public_figures[idx], jobId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ee17465-9cc4-4d08-a8c7-586fb187a529",
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    results = pool.starmap(create_embedding, inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d651ca37-71de-4e50-a6c1-975c75144364",
   "metadata": {},
   "source": [
    "# Augment Shot with Additional Details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc8daa5-bbb4-4781-80af-b11dcee4b9b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6cd8d7-8075-4f9d-8bc6-8065c7351d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_frames_public_figures[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b432f2ca-f2e9-460e-9066-6a5919bfe281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment_celebrity_detection_with_embeddings(shot_frames, index):\n",
    "    augmented_shot_frames = []\n",
    "    shot_publicFigures = set()\n",
    "\n",
    "    def from_set_to_str(str):\n",
    "        if not str:\n",
    "            return \"\"\n",
    "        else:\n",
    "            return \", \".join(str)\n",
    "\n",
    "    for _, value in enumerate(shot_frames):\n",
    "        frame_publicFigures = set()\n",
    "        for name in value[\"frame_publicFigures\"].split(','):\n",
    "            name = name.strip()\n",
    "            if name:\n",
    "                frame_publicFigures.add(name)\n",
    "                shot_publicFigures.add(name)       \n",
    "\n",
    "        embedding = get_titan_image_embedding(embedding_model, f\"{value['frame']}.png\")\n",
    "\n",
    "        query = {\n",
    "            \"size\": 100,\n",
    "            \"query\": {\"knn\": {\"frame_image_vector\": {\"vector\": embedding, \"k\": 100}}},\n",
    "            \"_source\": [\n",
    "                \"jobId\",\n",
    "                \"video_name\",\n",
    "                \"shot_startTime\",\n",
    "                \"shot_endTime\",\n",
    "                \"frame_publicFigures\"],\n",
    "        }\n",
    "        response = opensearch_client.search(body=query, index=index)\n",
    "        hits = response[\"hits\"][\"hits\"]\n",
    "        for hit in hits:\n",
    "            if hit[\"_score\"] >= 0.7:\n",
    "                public_figures = [name.strip() for name in hit[\"_source\"][\"frame_publicFigures\"].split(',')]\n",
    "                for name in public_figures:\n",
    "                    if name:\n",
    "                        frame_publicFigures.add(name.lower())\n",
    "                        shot_publicFigures.add(name.lower())\n",
    "\n",
    "        frame_publicFigures = from_set_to_str(frame_publicFigures)\n",
    "        augmented_shot_frames.append({\n",
    "            \"frame\": value[\"frame\"],\n",
    "            \"frame_publicFigures\": frame_publicFigures\n",
    "        })\n",
    "\n",
    "    shot_publicFigures = from_set_to_str(shot_publicFigures)\n",
    "    return augmented_shot_frames, shot_publicFigures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9504b733-5f16-4a19-a20b-4a15ccb679df",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_augment_celebrity_detection_with_embeddings = []\n",
    "for idx in range(len(shot_frames_public_figures)):\n",
    "  inputs_augment_celebrity_detection_with_embeddings.append((shot_frames_public_figures[idx], jobId))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26103de7-d1e7-4686-9fa1-0e6698c8b55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    augmented_celebrity_detection_results = pool.starmap(augment_celebrity_detection_with_embeddings, inputs_augment_celebrity_detection_with_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20c78095-1047-419a-845b-7c7fe960b5a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmented_celebrity_detection_results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf5aae1-e8b2-4889-9425-402930ccc85a",
   "metadata": {},
   "source": [
    "## Retrieve video transcription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcbf6e-a367-495c-8c40-858e61c81257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcript(s):\n",
    "    subtitle_blocks = re.findall(\n",
    "        r\"(\\d+\\n(\\d{2}:\\d{2}:\\d{2},\\d{3}) --> (\\d{2}:\\d{2}:\\d{2},\\d{3})\\n(.*?)(?=\\n\\d+\\n|\\Z))\",\n",
    "        s,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    sentences = [block[3].replace(\"\\n\", \" \").strip() for block in subtitle_blocks]\n",
    "    startTimes = [block[1] for block in subtitle_blocks]\n",
    "    endTimes = [block[2] for block in subtitle_blocks]\n",
    "    startTimes_ms = [time_to_ms(time) for time in startTimes]\n",
    "    endTimes_ms = [time_to_ms(time) for time in endTimes]\n",
    "\n",
    "    filtered_sentences = []\n",
    "    filtered_startTimes_ms = []\n",
    "    filtered_endTimes_ms = []\n",
    "\n",
    "    startTime_ms = -1\n",
    "    endTime_ms = -1\n",
    "    sentence = \"\"\n",
    "    for i in range(len(sentences)):\n",
    "        if startTime_ms == -1:\n",
    "            startTime_ms = startTimes_ms[i]\n",
    "        sentence += \" \" + sentences[i]\n",
    "        if (\n",
    "            sentences[i].endswith(\".\")\n",
    "            or sentences[i].endswith(\"?\")\n",
    "            or sentences[i].endswith(\"!\")\n",
    "            or i == len(sentences) - 1\n",
    "        ):\n",
    "            endTime_ms = endTimes_ms[i]\n",
    "            filtered_sentences.append(sentence.strip())\n",
    "            filtered_startTimes_ms.append(startTime_ms)\n",
    "            filtered_endTimes_ms.append(endTime_ms)\n",
    "            startTime_ms = -1\n",
    "            endTime_ms = -1\n",
    "            sentence = \"\"\n",
    "\n",
    "    processed_transcript = []\n",
    "    for i in range(len(filtered_sentences)):\n",
    "        processed_transcript.append(\n",
    "            {\n",
    "                \"sentence_startTime\": filtered_startTimes_ms[i],\n",
    "                \"sentence_endTime\": filtered_endTimes_ms[i],\n",
    "                \"sentence\": filtered_sentences[i],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return processed_transcript\n",
    "\n",
    "def time_to_ms(time_str):\n",
    "    h, m, s, ms = re.split(\":|,\", time_str)\n",
    "    return int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57b6a76-4f2c-4b84-92a1-2b799b698a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subtitle(bucket_transcripts, transcript_filename):\n",
    "    subtitle = (\n",
    "        s3_client.get_object(Bucket=bucket_transcripts, Key=transcript_filename)[\"Body\"]\n",
    "        .read()\n",
    "        .decode(\"utf-8-sig\")\n",
    "    )\n",
    "    return subtitle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080549d-e492-45a4-801c-5a311e00eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shot_transcript(shot_startTime, shot_endTime, transcript):\n",
    "    relevant_transcript = \"\"\n",
    "    for item in transcript:\n",
    "        if item[\"sentence_startTime\"] >= shot_endTime:\n",
    "            break\n",
    "        if item[\"sentence_endTime\"] <= shot_startTime:\n",
    "            continue\n",
    "        delta_start = max(item[\"sentence_startTime\"], shot_startTime)\n",
    "        delta_end = min(item[\"sentence_endTime\"], shot_endTime)\n",
    "        if delta_end - delta_start >= 500:\n",
    "            relevant_transcript += item[\"sentence\"] + \"; \"\n",
    "    return relevant_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a356d79d-6884-438b-8a41-913dc7e56ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "subtitle = get_subtitle(bucket_transcripts, jobId + \".srt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69f359c-bbd4-4912-9978-07a1c40126cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_transcript = process_transcript(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34349e7c-1135-4956-8819-74d60dcb5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "shots_transcript = []\n",
    "for shot in shots:\n",
    "    relevant_transcript = add_shot_transcript(shot['shot_startTime'], shot['shot_endTime'], processed_transcript)\n",
    "    shots_transcript.append(relevant_transcript)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d60506-0258-4a62-9c44-8b88e79d1d18",
   "metadata": {},
   "source": [
    "## Create the Shot Description \n",
    "For given images belong to a shot whithin a video, leverage an LLM to extract key elements from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4408a765-b6af-4278-bc30-db602c839f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "shot_description_llm = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "accept = \"application/json\"\n",
    "content_type = \"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e8b88-2b67-4ce8-ac69-6b9dfe2f30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shot_description(shot_frames, shot_transcript):\n",
    "    res = []\n",
    "    prompt = f\"\"\"Provide a detailed but concise description of a video shot based on the given frame images. Focus on creating a cohesive narrative of the entire shot rather than describing each frame individually. If the images contain frames from multiple shots, concentrate on describing the most prominent or central shot.\n",
    "\n",
    "        Before describing the shot:\n",
    "\n",
    "        - Identify the primary shot among the given frames.\n",
    "        - Disregard any frames that appear to belong to previous or next shots.\n",
    "        - If uncertain about which frames belong to the current shot, describe only the elements that are consistent across multiple frames.\n",
    "        \n",
    "        Then, incorporate the following elements in your description: \n",
    "        1. Visual elements:\n",
    "        - Describe all visible objects, text, and characters in detail.\n",
    "        - For any characters present, include:\n",
    "            • Age\n",
    "            • Emotional expressions\n",
    "            • Clothing and accessories\n",
    "            • Physical appearance\n",
    "            • Any actions, movements or gestures\n",
    "\n",
    "        2. Setting and atmosphere:\n",
    "        - Provide details about the time, location, and overall ambiance.\n",
    "        - Mention any relevant background elements that contribute to the scene.\n",
    "\n",
    "        3. Incorporate provided information:\n",
    "        - Seamlessly integrate details about public figures and private figures if available.\n",
    "        - If this information is not provided, rely solely on the visual elements.\n",
    "\n",
    "        Skip the preamble; go straight into the description.\"\"\"\n",
    "\n",
    "    for index, value in enumerate(shot_frames):\n",
    "        prompt += f\"Frame {index}: Public figures: {value['frame_publicFigures']}\\n\"\n",
    "\n",
    "    model_id = shot_description_llm\n",
    "    body = {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 512\n",
    "    }\n",
    "    for index, value in enumerate(shot_frames):\n",
    "\n",
    "        frame_path = os.path.join(tmp_frames_dir, f\"{value['frame']}.png\")\n",
    "        with open(frame_path, \"rb\") as image_file:\n",
    "            base64_image_string = base64.b64encode(image_file.read()).decode('utf8')\n",
    "\n",
    "        accept = \"application/json\"\n",
    "        content_type = \"application/json\"\n",
    "        body[\"messages\"][0][\"content\"].append({\n",
    "            \"type\": \"image\",\n",
    "            \"source\": {\n",
    "                \"type\": \"base64\",\n",
    "                \"media_type\": \"image/png\",\n",
    "                \"data\": base64_image_string,\n",
    "            },\n",
    "        })\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=json.dumps(body), modelId=model_id, accept=accept, contentType=content_type\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    response_body = response_body[\"content\"][0][\"text\"]\n",
    "\n",
    "    return response_body"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca072a9b-c287-4fb2-8a42-be897a4a977a",
   "metadata": {},
   "source": [
    "The following step take a while to complete. (> 20 minutes). Consider using a smaller model (e.g. Haiku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc39e1a-d9f1-432c-b3e2-e8ea879b5165",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "shots_description = []\n",
    "\n",
    "for idx, shot_frames in enumerate(shot_frames_public_figures):\n",
    "    shot_description = get_shot_description(shot_frames, shots_transcript[idx])\n",
    "    shots_description.append(shot_description)\n",
    "    time.sleep(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d186e9-89d1-48b4-9b6b-91dc468a4b92",
   "metadata": {},
   "source": [
    "## Generate Embeddings for Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091e6938-98ff-4594-9521-df5da5137079",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(text_embedding_model, text):\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    if text_embedding_model.startswith(\"amazon.titan-embed-text\"):\n",
    "        body = json.dumps({\"inputText\": text, \"dimensions\": 1024, \"normalize\": True})\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=text_embedding_model,\n",
    "            accept=accept,\n",
    "            contentType=content_type,\n",
    "        )\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        embedding = response_body.get(\"embedding\")\n",
    "    else:\n",
    "        if len(text) > 2048:\n",
    "            text = text[:2048]\n",
    "        body = json.dumps({\"texts\": [text], \"input_type\": \"search_document\"})\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body,\n",
    "            modelId=text_embedding_model,\n",
    "            accept=accept,\n",
    "            contentType=content_type,\n",
    "        )\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        embedding = response_body.get(\"embeddings\")[0]\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbedccab-ef2c-4d79-ba81-ae7975bae754",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_embedding_model = \"amazon.titan-embed-text-v2:0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "222dc5f1-7746-4135-a5c7-1c8fc46a8d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_get_text_embedding = []\n",
    "for shot_description in shots_description:\n",
    "  inputs_get_text_embedding.append((text_embedding_model, shot_description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11e27023-5418-4181-9c3a-a1d2233dbc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    shots_text_embedding = pool.starmap(get_text_embedding, inputs_get_text_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1493d2fd-a51a-45c5-8caa-51365a4f9ceb",
   "metadata": {},
   "source": [
    "## Generate Embeddings for Shot Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999bdb93-3c15-4e6b-b7da-6869cddb5912",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_image_embedding(bucket, jobId, image):\n",
    "    s3_object = s3_client.get_object(Bucket=bucket, Key=f\"{jobId}/shots/{image}.png\")\n",
    "    image_content = s3_object[\"Body\"].read()\n",
    "    base64_image_string = base64.b64encode(image_content).decode()\n",
    "\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    body = json.dumps({\"inputImage\": base64_image_string})\n",
    "\n",
    "    response = bedrock_client.invoke_model(\n",
    "        body=body,\n",
    "        modelId=embedding_model,\n",
    "        accept=accept,\n",
    "        contentType=content_type,\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "    return embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed1b332-d0cf-4ec4-a5bd-df69585d0b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_get_shot_image_embedding = []\n",
    "for shot_id in shot_ids:\n",
    "  inputs_get_shot_image_embedding.append((bucket_shots, jobId, shot_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f0a707-812d-485d-877f-adfd32ceae6b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with multiprocessing.Pool() as pool:\n",
    "    shots_image_embedding = pool.starmap(get_image_embedding, inputs_get_shot_image_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53d953-ea15-4d0f-9636-2e222e6303dd",
   "metadata": {},
   "source": [
    "## Ingest Embeddings into Vector Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8c457c0-0489-4f4a-8022-0d9fa46d3c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, shot_id in enumerate(shot_ids):\n",
    "    shot_startTime = shots[idx]['shot_startTime']\n",
    "    embedding_request_body = json.dumps(\n",
    "            {\n",
    "                \"jobId\": jobId,\n",
    "                \"video_name\": video_name,\n",
    "                \"shot_id\": shot_id,\n",
    "                \"shot_startTime\": shots[idx]['shot_startTime'],\n",
    "                \"shot_endTime\": shots[idx]['shot_endTime'],\n",
    "                \"shot_description\": shots_description[idx],\n",
    "                \"shot_publicFigures\": augmented_celebrity_detection_results[idx][1],\n",
    "                \"shot_transcript\": shots_transcript[idx],\n",
    "                \"shot_desc_vector\": shots_text_embedding[idx],\n",
    "                \"shot_image_vector\": shots_image_embedding[idx],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    response = opensearch_client.index(\n",
    "                index=open_search_index,\n",
    "                body=embedding_request_body,\n",
    "                params={\"timeout\": 60},\n",
    "            )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3fc17-e5c4-42e7-ba00-eff91e92ee5f",
   "metadata": {},
   "source": [
    "## Perform Video Semantic Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d0b35e4-c3cc-4c15-9e05-432bb699f941",
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehend_client = boto3.client(\"comprehend\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a8fa9-4d03-47f3-917e-9fc9a93fc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Scott driving a car\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e33248b-c02e-4656-858a-e974cf9c178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = comprehend_client.detect_entities(Text=user_query, LanguageCode=\"en\")\n",
    "names = []\n",
    "locations = []\n",
    "for entity in response[\"Entities\"]:\n",
    "    if entity[\"Type\"] == \"PERSON\":\n",
    "        names.append(entity[\"Text\"])\n",
    "\n",
    "entities = names + locations\n",
    "if entities:\n",
    "    entities = \", \".join(entities)\n",
    "else:\n",
    "    entities = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92fa877-93b3-4323-94fe-342839875e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "910ac01a-c027-45a5-86e0-4ddc5b9b10ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query_text_embedding = get_text_embedding(text_embedding_model, user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bee18-b641-4a08-a9fc-a6c731214630",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\"(.*?)\"'\n",
    "matches = re.findall(pattern, user_query)\n",
    "matches = \",\".join(matches) if matches else \"\"\n",
    "\n",
    "aoss_query = {\n",
    "        \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"bool\": {\"should\": []}},\n",
    "                \"script\": {\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"shot_desc_vector\",\n",
    "                        \"query_value\": user_query_text_embedding,\n",
    "                        \"space_type\": \"cosinesimil\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"jobId\",\n",
    "            \"video_name\",\n",
    "            \"shot_id\",\n",
    "            \"shot_startTime\",\n",
    "            \"shot_endTime\",\n",
    "            \"shot_description\",\n",
    "            \"shot_publicFigures\",\n",
    "            \"shot_transcript\",\n",
    "        ],\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f4f5cf-3f4d-4194-9beb-47e7eb7179a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "pattern = r'\"(.*?)\"'\n",
    "matches = re.findall(pattern, user_query)\n",
    "if len(matches) > 0:\n",
    "    aoss_query[\"query\"][\"script_score\"][\"query\"][\"bool\"][\"must\"] = []\n",
    "    for match in matches:\n",
    "        aoss_query[\"query\"][\"script_score\"][\"query\"][\"bool\"][\"must\"].append(\n",
    "            {\n",
    "                \"multi_match\": {\n",
    "                    \"query\": match,\n",
    "                    \"fields\": [\n",
    "                        \"shot_publicFigures\",\n",
    "                        \"shot_privateFigures\",\n",
    "                        \"shot_description\",\n",
    "                    ],\n",
    "                    \"type\": \"phrase\",\n",
    "                }\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1323c3c-3dc0-4745-94cf-91db90ab32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = opensearch_client.search(body=aoss_query, index=open_search_index)\n",
    "hits = response[\"hits\"][\"hits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "952f3b81-58a7-4f1a-946a-ae08e23de9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "responses = []\n",
    "for hit in hits:\n",
    "        if hit[\"_score\"] >= 0:  # Set score threshold\n",
    "            responses.append(\n",
    "                {\n",
    "                    \"jobId\": hit[\"_source\"][\"jobId\"],\n",
    "                    \"video_name\": hit[\"_source\"][\"video_name\"],\n",
    "                    \"shot_id\": hit[\"_source\"][\"shot_id\"],\n",
    "                    \"shot_startTime\": hit[\"_source\"][\"shot_startTime\"],\n",
    "                    \"shot_endTime\": hit[\"_source\"][\"shot_endTime\"],\n",
    "                    \"shot_description\": hit[\"_source\"][\"shot_description\"],\n",
    "                    \"shot_publicFigures\": hit[\"_source\"][\"shot_publicFigures\"],\n",
    "                    \"shot_transcript\": hit[\"_source\"][\"shot_transcript\"],\n",
    "                    \"score\": hit[\"_score\"],\n",
    "                }\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747dc248-915c-47c2-a0f0-986ab82f593f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Video\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651fbe9f-5d65-4bae-87ac-efc9bf17db36",
   "metadata": {},
   "source": [
    "## Shows the top 2 search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a1096-0014-42c3-887a-6f8aa5089253",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_results = 2\n",
    "for result_index in range(top_results):\n",
    "    video_start=responses[result_index]['shot_startTime']/1000\n",
    "    video_end=responses[result_index]['shot_endTime']/1000\n",
    "    video_name = responses[result_index]['video_name']\n",
    "    converted_start = str(datetime.timedelta(seconds = video_start))\n",
    "    converted_end = str(datetime.timedelta(seconds = video_end))\n",
    "    _ = subprocess.run(\n",
    "        [\n",
    "            ffmpeg_path,\n",
    "            \"-ss\",\n",
    "            converted_start,\n",
    "            \"-to\",\n",
    "            converted_end,\n",
    "            \"-i\",\n",
    "            video_name,\n",
    "            \"-c\",\n",
    "            \"copy\",\n",
    "            f\"{result_index}.mp4\",\n",
    "        ],\n",
    "        stderr=subprocess.PIPE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af6537-11c2-466f-a8ce-bc2605d0e88d",
   "metadata": {},
   "source": [
    "## Semantic Search Result #1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9032f-a99f-440a-b530-8a626f8b15a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(url=\"0.mp4\", width=640, height=360)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f7b91d-9fb4-48a8-a7e6-07d4a335d6a2",
   "metadata": {},
   "source": [
    "## Semantic Search Result #2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76171b37-836c-431e-b198-b901f15b8132",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(url=\"1.mp4\", width=640, height=360)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
