{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a8458ac4-23ed-461e-bf44-55432f709d1a",
   "metadata": {},
   "source": [
    "# Semantic Video Search\n",
    "\n",
    "Semantic video search is an advanced technology that enables users to find and retrieve video content based on its meaning and context, rather than relying solely on keywords or metadata. Content creators produce videos with rich, multidimensional information. The semantic search algorithm analyzes and understands this content using AI technologies like computer vision, natural language processing, and now, generative AI. This allows users to interact with the system, searching for specific concepts, objects, or actions within videos.\n",
    "\n",
    "![Semantic Video Search](./static/images/04-semantic-video-search.png)\n",
    "\n",
    "Semantic video search is crucial for the media and entertainment industry as it dramatically improves content discovery, user engagement, and the overall viewing experience. In an era of content overload, users demand more efficient ways to find relevant videos. Traditional search methods often fall short, leading to frustrated users and underutilized content libraries. Semantic search allows media companies to unlock the full potential of their video archives, improve recommendation systems, and create more personalized viewing experiences.\n",
    "\n",
    "However, implementing effective semantic video search comes with significant challenges. The sheer volume and complexity of video data make it difficult to analyze and index content accurately. Variations in video quality, language, and cultural context can lead to misinterpretations. Generative AI offers a promising solution to enhance semantic video search capabilities. By leveraging large language models and multimodal AI, generative AI can provide more nuanced and context-aware analysis of video content. It can generate detailed descriptions of scenes, identify complex actions and emotions, and even understand subtle cultural references, bridging the gap between user intent and video content.\n",
    "\n",
    "In this lab, you'll create a multi-modal vector database using visual and audio metadata generated from previously labs to build a multi-modal(MM) search database. By the end of the lab, you'll be able to qury against this database using natural language or images, and find quickly find the relevant shots from the video."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743be170-1b38-4ba1-b33b-f19841acd135",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0292f5c5-22ed-4d89-906c-a9424ccdf01f",
   "metadata": {},
   "source": [
    "To run this notebook, you need to have run the previous notebook:[01-video-time-segmentation](01-video-time-segmentation.ipynb), where you segmented the video using audio, visual and semantic information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "870de375-fcf0-46f4-9154-1b54fb7b7737",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c286d5-2bf1-4aff-99ca-74b9d929c3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import os\n",
    "import time\n",
    "import re\n",
    "from IPython.display import display, JSON, HTML\n",
    "import subprocess\n",
    "from PIL import Image\n",
    "import base64\n",
    "from termcolor import colored\n",
    "from opensearchpy import OpenSearch, RequestsHttpConnection, AWSV4SignerAuth\n",
    "import random\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2324a9e1-fbc5-47d3-81f1-24528f9dfba7",
   "metadata": {},
   "source": [
    "### Retrieve saved values from previous notebooks\n",
    "To run this notebook, you need to have run the previous notebook: 00_prerequisites.ipynb, where you installed package dependencies and gathered some information from the SageMaker environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13068f49-0a48-4d1a-ac1c-c504dc0a58f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f749e7-42c4-4c40-8079-6e180e32c74d",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_path = video['path']\n",
    "rek_client = boto3.client(\"rekognition\")\n",
    "bedrock_runtime = boto3.client(\"bedrock-runtime\")\n",
    "region = sagemaker_resources['region']\n",
    "oss_host = session['AOSSCollectionEndpoint']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d8ae5c-a196-4906-8c85-66ca6002ec64",
   "metadata": {},
   "source": [
    "# Architecture\n",
    "\n",
    "This hands-on workflow uses AWS services from SageMaker.  It takes video frame, shots, and audio transcription as inputs and produces a seachable vector index in opesearch serverless.\n",
    "\n",
    "[TBD]()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98928c85-38f7-49f1-b548-0d15d01f19ae",
   "metadata": {},
   "source": [
    "## Randomly Sample a subset of shots from the video\n",
    "\n",
    "For a better and uninterrupted lab experience, we will randomly sample 20 shots from the original videos in chronical order. This approach is necessary due to the limited capacity in the workshop environment. Do you so will still maintain the integrity of the exercise while ensuring that all participants can complete the lab without getting throttled. If you are not in the workshop sandbox environment, please feel free to increase the number of shots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de8d2ea-8858-4c24-87a6-c9c108826186",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def sample_shots(original_list, sample_size=10):\n",
    "    # Check if the sample size is larger than the list\n",
    "    if sample_size >= len(original_list):\n",
    "        return original_list\n",
    "\n",
    "    # Create a set of random indices\n",
    "    indices = set(random.sample(range(len(original_list)), sample_size))\n",
    "    \n",
    "    # Use a list comprehension to select items at these indices\n",
    "    return [item for i, item in enumerate(original_list) if i in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc1a9f7-d276-43f4-8235-c10abfeb756e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sampled_shots = sample_shots(video['shots'].shots, sample_size=20)\n",
    "\n",
    "for shot in sampled_shots:\n",
    "    print(f\"Sampled shot id: {shot['id']} ===================\\n\")\n",
    "    display(Image.open(shot['composite_images'][0]['file']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351d55a-0263-4500-9d0a-f9314fbcb4c2",
   "metadata": {},
   "source": [
    "## Detect Celebrities Using Amazon Rekognition\n",
    "[Amazon Rekognition](https://aws.amazon.com/rekognition/) can be used to recognize international, widely known celebrities like actors, sportspeople, and online content creators. The metadata provided by the celebrity recognition API significantly reduces the repetitive manual effort required to tag content and make it readily searchable. In the following section, we'll leverage this feature to help us detect any celebrities in the shots extracted in the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "400dce05-ad3a-4751-9f58-4d3ac885bcc6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def detect_celebrities(shot):\n",
    "    start_frame_id = shot['start_frame_id']\n",
    "    end_frame_id = shot['end_frame_id']\n",
    "    video_asset_dir = shot['video_asset_dir']\n",
    "\n",
    "    frames = range(start_frame_id, end_frame_id + 1)\n",
    "\n",
    "    celebrities = set()\n",
    "\n",
    "    for frame_id in frames:\n",
    "        try:\n",
    "            #image_path = f\"{video_asset_dir}/frames/frames{frame_id+1:07d}.jpg\"\n",
    "            image_path = f\"{video_asset_dir}/frames/frames{frame_id+1:07d}.jpg\"\n",
    "            with open(image_path, 'rb') as image_file:\n",
    "                image_bytes = image_file.read()      \n",
    "\n",
    "            # Call Rekognition to detect celebrities\n",
    "            response = rek_client.recognize_celebrities(\n",
    "                Image={'Bytes': image_bytes}\n",
    "            )\n",
    "\n",
    "            min_confidence = 95.0 # change this value if the accuracy is low.\n",
    "\n",
    "            for celebrity in response.get('CelebrityFaces', []):\n",
    "                if celebrity.get('MatchConfidence', 0.0) >= min_confidence:\n",
    "                    celebrities.add(celebrity['Name'])\n",
    "\n",
    "        except ClientError as e:\n",
    "            pass\n",
    "\n",
    "    public_figures = ', '.join(celebrities)\n",
    "\n",
    "    shot[\"public_figure\"] = public_figures\n",
    "    \n",
    "    return {\n",
    "            \"shot_id\": shot['id'],\n",
    "            \"public_figure\": public_figures\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5207574-8777-4170-94fc-3792dc59b130",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"===== [Celebrities detected in each shot] ======\\n\")\n",
    "for shot in sampled_shots:\n",
    "    print(detect_celebrities(shot))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3257c5c-eb42-4950-8470-21c47a68b42e",
   "metadata": {},
   "source": [
    "## Process Audio Transcription\n",
    "\n",
    "Convert close caption to timestamp with complete sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bcbf6e-a367-495c-8c40-858e61c81257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcript(s):\n",
    "    subtitle_blocks = re.findall(\n",
    "        r\"(\\d+\\n(\\d{2}:\\d{2}:\\d{2}.\\d{3}) --> (\\d{2}:\\d{2}:\\d{2}.\\d{3})\\n(.*?)(?=\\n\\d+\\n|\\Z))\",\n",
    "        s,\n",
    "        re.DOTALL,\n",
    "    )\n",
    "\n",
    "    sentences = [block[3].replace(\"\\n\", \" \").strip() for block in subtitle_blocks]\n",
    "    startTimes = [block[1] for block in subtitle_blocks]\n",
    "    endTimes = [block[2] for block in subtitle_blocks]\n",
    "\n",
    "    startTimes_ms = [time_to_ms(time) for time in startTimes]\n",
    "    endTimes_ms = [time_to_ms(time) for time in endTimes]\n",
    "\n",
    "    filtered_sentences = []\n",
    "    filtered_startTimes_ms = []\n",
    "    filtered_endTimes_ms = []\n",
    "\n",
    "    startTime_ms = -1\n",
    "    endTime_ms = -1\n",
    "    sentence = \"\"\n",
    "    for i in range(len(sentences)):\n",
    "        if startTime_ms == -1:\n",
    "            startTime_ms = startTimes_ms[i]\n",
    "        sentence += \" \" + sentences[i]\n",
    "        if (\n",
    "            sentences[i].endswith(\".\")\n",
    "            or sentences[i].endswith(\"?\")\n",
    "            or sentences[i].endswith(\"!\")\n",
    "            or i == len(sentences) - 1\n",
    "        ):\n",
    "            endTime_ms = endTimes_ms[i]\n",
    "            filtered_sentences.append(sentence.strip())\n",
    "            filtered_startTimes_ms.append(startTime_ms)\n",
    "            filtered_endTimes_ms.append(endTime_ms)\n",
    "            startTime_ms = -1\n",
    "            endTime_ms = -1\n",
    "            sentence = \"\"\n",
    "\n",
    "    processed_transcript = []\n",
    "    for i in range(len(filtered_sentences)):\n",
    "        processed_transcript.append(\n",
    "            {\n",
    "                \"sentence_startTime\": filtered_startTimes_ms[i],\n",
    "                \"sentence_endTime\": filtered_endTimes_ms[i],\n",
    "                \"sentence\": filtered_sentences[i],\n",
    "            }\n",
    "        )\n",
    "\n",
    "    return processed_transcript\n",
    "\n",
    "def time_to_ms(time_str):\n",
    "    h, m, s, ms = re.split(r\"[:|.]\", time_str)\n",
    "    return int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f339e88a-4e83-4e1d-a3ad-486350f7de05",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with open(video['transcript'].vtt_file, encoding=\"utf-8\") as f:\n",
    "    transcript = f.read()\n",
    "\n",
    "processed_transcript = process_transcript(transcript)\n",
    "\n",
    "processed_transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b0a6d-24f8-41f9-ae5a-de529fe02981",
   "metadata": {},
   "source": [
    "## Align sentences to shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0080549d-e492-45a4-801c-5a311e00eea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_shot_transcript(shot_startTime, shot_endTime, transcript):\n",
    "    relevant_transcript = \"\"\n",
    "    for item in transcript:\n",
    "        if item[\"sentence_startTime\"] >= shot_endTime:\n",
    "            break\n",
    "        if item[\"sentence_endTime\"] <= shot_startTime:\n",
    "            continue\n",
    "        delta_start = max(item[\"sentence_startTime\"], shot_startTime)\n",
    "        delta_end = min(item[\"sentence_endTime\"], shot_endTime)\n",
    "        if delta_end - delta_start >= 500:\n",
    "            relevant_transcript += item[\"sentence\"] + \"; \"\n",
    "    return relevant_transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34349e7c-1135-4956-8819-74d60dcb5155",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shot in sampled_shots:\n",
    "    relevant_transcript = add_shot_transcript(shot['start_ms'], shot['end_ms'], processed_transcript)\n",
    "    shot['transcript'] = relevant_transcript\n",
    "    print({\n",
    "        'shot_id': shot['id'],\n",
    "        'transcript': relevant_transcript\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66d60506-0258-4a62-9c44-8b88e79d1d18",
   "metadata": {},
   "source": [
    "## Create the Shot Description \n",
    "For given images belong to a shot whithin a video, leverage an LLM to extract key elements from the images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9e8b88-2b67-4ce8-ac69-6b9dfe2f30ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_shot_description(model_id, composite_images, celebrities):\n",
    "\n",
    "    system_prompts = [{\"text\": \"\"\"\n",
    "    You are an expert video content analyst specializing in generating rich, contextual metadata for semantic search systems. \n",
    "    Your task is to analyze video shots presented in a sequence of frame images and provide a detailed but concise description \n",
    "    of a video shot based on the given frame images. Focus on creating a cohesive narrative of the entire shot rather than \n",
    "    describing each frame individually.\n",
    "    \"\"\"}]\n",
    "    \n",
    "    prompt = \"\"\"\n",
    "    <celebrities>\n",
    "    {{CELEBRITIES}}\n",
    "    </celebrities>\n",
    "    \n",
    "   Context:\n",
    "    - Each image contains a sequence of consecutive video frames, read from left to right and top to bottom.\n",
    "    - Your goal is to generate metadata that makes the video content easily discoverable through various search queries.\n",
    "    - ALL identified <celebrities> MUST be integrated into descriptions.\n",
    "\n",
    "    STRICT VALIDATION REQUIREMENTS:\n",
    "    1. STOP AND CHECK BEFORE OUTPUTTING:\n",
    "       - Are there any names in the \"celebrities\" field? \n",
    "       - If YES, verify these names appear in the description text\n",
    "       - If NO match found, rewrite description to include celebrity names\n",
    "       \n",
    "    2. REQUIRED FORMAT FOR DESCRIPTIONS WITH CELEBRITIES:\n",
    "       - MUST start with celebrity names and their actions\n",
    "       - Example format: \"[Celebrity Name] appears/is shown/portrays...\"\n",
    "       - NEVER output generic terms (\"a man\", \"someone\") when celebrity identity is known\n",
    "    \n",
    "    3. AUTOMATIC ERROR CHECKING:\n",
    "       If (celebrities.length > 0):\n",
    "          If (description does not contain ALL celebrity names):\n",
    "             MUST rewrite description\n",
    "    \n",
    "    Description Template When Celebrities Present:\n",
    "    \"[Celebrity Name 1] [action/appearance], [clothing/setting details]. [Additional context]. [Celebrity Name 2 if present] [their action/appearance]...\"\n",
    "\n",
    "    REQUIRED PRE-SUBMISSION CHECKS:\n",
    "    □ Are all celebrity names from <celebrities> present in description?\n",
    "    □ Does description start with a celebrity name (not generic terms)?\n",
    "    □ Are all celebrities actively described (not passively mentioned)?\n",
    "    □ Have you avoided generic terms like \"a man\" or \"someone\"?\n",
    "    \n",
    "    INCORRECT (Reject):\n",
    "    \"A man in a white shirt and tie is shown...\"\n",
    "    (When celebrities field contains \"Kevin Kilner\")\n",
    "    \n",
    "    CORRECT (Accept):\n",
    "    \"Kevin Kilner appears in a white shirt and tie...\"\n",
    "    \n",
    "    Input Description:\n",
    "    <input_description>\n",
    "    - Sequence of images representing video frames\n",
    "    - List of known celebrities (if applicable)\n",
    "    </input_description>\n",
    "\n",
    "    Step-by-step Instructions:\n",
    "    <instructions>\n",
    "    1. Analyze the visual content:\n",
    "       a. First priority: Identify any celebrities or notable individuals\n",
    "       b. Check for dark/empty frames:\n",
    "          - If frames are black or empty, use specialized template\n",
    "          - Set appropriate technical descriptors\n",
    "          - Mark confidence scores as 100 for verified empty content\n",
    "          - Use \"None\" or \"Undefined\" for inapplicable categories\n",
    "       c. If celebrities identified, prepare description using required template\n",
    "       d. Identify key objects, actions, and settings in the scene\n",
    "       e. Detect any text or graphics visible in the frames\n",
    "       f. Recognize brands, logos, or products\n",
    "    \n",
    "    2. Determine temporal aspects:\n",
    "       a. Identify any scene transitions or significant changes in the sequence\n",
    "       b. Note any recurring elements across multiple frames\n",
    "    \n",
    "    3. Synthesize a detailed description:\n",
    "       a. REQUIRED: If celebrities present, use template format\n",
    "       b. MUST start with celebrity identification and actions\n",
    "       c. Integrate setting, atmosphere, and context\n",
    "       d. Include all identified celebrities in natural narrative flow\n",
    "       e. Run pre-submission checks before finalizing\n",
    "    \n",
    "    4. Final Validation:\n",
    "       a. Run through pre-submission checklist\n",
    "       b. Verify celebrity integration in description (if applicable)\n",
    "       c. Confirm no generic terms used for identified people\n",
    "       d. For dark frames, verify all technical descriptors are accurate\n",
    "    \n",
    "    5. Special Cases Handling:\n",
    "        a. For dark/empty frames:\n",
    "           - Use technical description template\n",
    "           - Set appropriate null values\n",
    "           - Mark relevant technical indicators\n",
    "           - Note possible transition purpose\n",
    "        b. For partially visible content:\n",
    "           - Note visibility issues\n",
    "           - Describe what can be confidently identified\n",
    "           - Adjust confidence scores accordingly\n",
    "    \n",
    "    6. Final Output Preparation:\n",
    "        a. Skip the preamble; go straight into the description\n",
    "        b. Check for proper formatting and syntax\n",
    "    \"\"\".replace(\"{{CELEBRITIES}}\", celebrities)\n",
    "\n",
    "    \n",
    "    message = {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":[]\n",
    "    }\n",
    "                         \n",
    "    for composite in composite_images:\n",
    "\n",
    "        with open(composite['file'], \"rb\") as image_file:\n",
    "            image_string = image_file.read()\n",
    "\n",
    "        message[\"content\"].append({\n",
    "            \"image\":{\n",
    "                \"format\": \"jpeg\",\n",
    "                \"source\":{\n",
    "                    \"bytes\": image_string\n",
    "                }\n",
    "            }\n",
    "                \n",
    "        })\n",
    "\n",
    "    message[\"content\"].append({\n",
    "        \"text\": prompt\n",
    "    })\n",
    "    \n",
    "    # Base inference parameters to use.\n",
    "    inference_config = {\"temperature\": .1}\n",
    "    \n",
    "    # Additional inference parameters to use.\n",
    "    additional_model_fields = {\"top_k\": 200}\n",
    "    \n",
    "    response = bedrock_runtime.converse(\n",
    "        modelId=model_id,\n",
    "        messages=[message],\n",
    "        system=system_prompts,\n",
    "        inferenceConfig=inference_config,\n",
    "        additionalModelRequestFields=additional_model_fields\n",
    "    )\n",
    "    output_message = response['output']['message']\n",
    "    \n",
    "    return output_message[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca072a9b-c287-4fb2-8a42-be897a4a977a",
   "metadata": {},
   "source": [
    "The following step take a while to complete. (> 20 minutes). Consider using a smaller model (e.g. Haiku)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a08d87b-2533-4823-a3b8-b4a2edd229ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model_id = 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "\n",
    "for shot in sampled_shots:\n",
    "    description = get_shot_description(\n",
    "        model_id = model_id, \n",
    "        composite_images = shot['composite_images'], \n",
    "        celebrities = shot['public_figure']\n",
    "    )\n",
    "    shot['shot_description'] = description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d186e9-89d1-48b4-9b6b-91dc468a4b92",
   "metadata": {},
   "source": [
    "## Generate Embeddings for Shots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4177f3fe-2ae5-4bf7-8b28-17014b6e52f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(model_id, input_data):\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "\n",
    "    if 'text' in model_id:\n",
    "        body = json.dumps({\n",
    "            \"inputText\": input_data,\n",
    "            \"dimensions\": 1024,\n",
    "            \"normalize\": True\n",
    "        })\n",
    "    elif 'image' in model_id:\n",
    "        # Read image from file and encode it as base64 string.\n",
    "        with open(input_data, \"rb\") as image_file:\n",
    "            input_image = base64.b64encode(image_file.read()).decode('utf8')\n",
    "        \n",
    "        body = json.dumps({\n",
    "            \"inputImage\": input_image,\n",
    "            \"embeddingConfig\": {\n",
    "                \"outputEmbeddingLength\": 1024\n",
    "            }\n",
    "        })\n",
    "    else:\n",
    "        raise ValueError(\"Invalid embedding_type. Choose 'text' or 'image'.\")\n",
    "\n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        body=body,\n",
    "        modelId=model_id,\n",
    "        accept=accept,\n",
    "        contentType=content_type,\n",
    "    )\n",
    "    response_body = json.loads(response[\"body\"].read())\n",
    "    embedding = response_body.get(\"embedding\")\n",
    "\n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de53d953-ea15-4d0f-9636-2e222e6303dd",
   "metadata": {},
   "source": [
    "## Building the OpenSearch Serverless Vector Index\n",
    "\n",
    "OpenSearch Serverless (OSS) is a fully managed, on-demand search and analytics service provided by Amazon Web Services (AWS). It allows users to deploy, operate, and scale OpenSearch clusters without the need for infrastructure management.\n",
    "\n",
    "An index in OpenSearch is a collection of documents that share similar characteristics. In this case, we're focusing on a vector index, which is designed to store and search vector embeddings efficiently.\n",
    "\n",
    "### Here is the Index Configuration\n",
    "\n",
    "The index contain following attributes:\n",
    "- `video_path`: Path to the video file (text field)\n",
    "- `shot_id`: Unique identifier for each shot (text field)\n",
    "- `shot_startTime`: Start time of the shot (text field)\n",
    "- `shot_endTime`: End time of the shot (text field)\n",
    "- `shot_description`: Description of the shot (text field)\n",
    "- `shot_celebrities`: Celebrities identified in the shot (text field)\n",
    "- `shot_transcript`: Audo Transcript of the shot (text field)\n",
    "\n",
    "These are metadata fileds we can use retrive for each search query as well as use them to filter down results. \n",
    "\n",
    "- `shot_image_vector`: Vector representation of the shot image\n",
    "- `shot_desc_vector`: Vector representation of the shot description\n",
    "\n",
    "Both `shot_image_vector` and `shot_desc_vector` are configured as `knn_vector` fields. you will use these two field to conduct vector similarity search to find the closest matching camera shot corresponding to your text query or input image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c63d4371-bd0f-4cf3-bb8c-fc8009ef4d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish client connection OSS\n",
    "def get_opensearch_client(host, region):\n",
    "    host = host.split(\"://\")[1] if \"://\" in host else host\n",
    "    credentials = boto3.Session().get_credentials()\n",
    "    auth = AWSV4SignerAuth(credentials, region, \"aoss\")\n",
    "\n",
    "    oss_client = OpenSearch(\n",
    "        hosts=[{\"host\": host, \"port\": 443}],\n",
    "        http_auth=auth,\n",
    "        use_ssl=True,\n",
    "        verify_certs=True,\n",
    "        connection_class=RequestsHttpConnection,\n",
    "        pool_maxsize=20,\n",
    "    )\n",
    "\n",
    "    return oss_client\n",
    "\n",
    "\n",
    "# Create Opensearch Severless Index\n",
    "def create_opensearch_index(oss_client, index_name, len_embedding=1024):\n",
    "\n",
    "    exist = oss_client.indices.exists(index_name)\n",
    "    if not exist:\n",
    "        print(\"Creating index\")\n",
    "        index_body = {\n",
    "            \"mappings\": {\n",
    "                \"properties\": {\n",
    "                    \"video_path\": {\"type\": \"text\"},\n",
    "                    \"shot_id\": {\"type\": \"text\"},\n",
    "                    \"shot_startTime\": {\"type\": \"text\"},\n",
    "                    \"shot_endTime\": {\"type\": \"text\"},\n",
    "                    \"shot_description\": {\"type\": \"text\"},\n",
    "                    \"shot_celebrities\": {\"type\": \"text\"},\n",
    "                    \"shot_transcript\": {\"type\": \"text\"},\n",
    "                    \"shot_image_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                    \"shot_desc_vector\": {\n",
    "                        \"type\": \"knn_vector\",\n",
    "                        \"dimension\": len_embedding,\n",
    "                        \"method\": {\n",
    "                            \"engine\": \"nmslib\",\n",
    "                            \"space_type\": \"cosinesimil\",\n",
    "                            \"name\": \"hnsw\",\n",
    "                            \"parameters\": {\"ef_construction\": 512, \"m\": 16},\n",
    "                        },\n",
    "                    },\n",
    "                }\n",
    "            },\n",
    "            \"settings\": {\n",
    "                \"index\": {\n",
    "                    \"number_of_shards\": 2,\n",
    "                    \"knn.algo_param\": {\"ef_search\": 512},\n",
    "                    \"knn\": True,\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "        response = oss_client.indices.create(index_name, body=index_body)\n",
    "\n",
    "        print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fbfcdb-00f2-4d15-adcb-5f63a3d7a439",
   "metadata": {},
   "outputs": [],
   "source": [
    "index_name = \"video_search_index\"\n",
    "\n",
    "oss_client = get_opensearch_client(oss_host, region)\n",
    "create_opensearch_index(oss_client, index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1119aa56-0a5f-4caa-a534-995f5fc8c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for shot in sampled_shots:\n",
    "\n",
    "    # generate text embedding from description\n",
    "    shot_desc_vector = get_embedding(\n",
    "        model_id='amazon.titan-embed-text-v2:0',\n",
    "        input_data=shot['shot_description']\n",
    "    )\n",
    "    \n",
    "    # generate mm embedding from composite frames\n",
    "    shot_image_vector = get_embedding(\n",
    "        model_id='amazon.titan-embed-image-v1',\n",
    "        input_data=shot['composite_images'][0]['file']\n",
    "    )\n",
    "\n",
    "    #build the payload to index in OSS\n",
    "    payload = json.dumps(\n",
    "            {\n",
    "                \"video_path\": video_path,\n",
    "                \"shot_id\": shot['id'],\n",
    "                \"shot_startTime\": shot['start_ms'],\n",
    "                \"shot_endTime\": shot['end_ms'],\n",
    "                \"shot_description\": shot['shot_description'],\n",
    "                \"shot_celebrities\": shot['public_figure'],\n",
    "                \"shot_transcript\": shot['transcript'],\n",
    "                \"shot_desc_vector\": shot_desc_vector,\n",
    "                \"shot_image_vector\": shot_image_vector,\n",
    "            }\n",
    "        )\n",
    "    response = oss_client.index(\n",
    "                    index=index_name,\n",
    "                    body=payload,\n",
    "                    params={\"timeout\": 60},\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68d3fc17-e5c4-42e7-ba00-eff91e92ee5f",
   "metadata": {},
   "source": [
    "## Perform Video Semantic Search\n",
    "\n",
    "But we will first make sure the inserted data in OpenSearch is ready to be searched."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1073b6ac-a09c-4a36-bd65-357671105ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Waiting for the recent inserted data to be searchable in OpenSearch...\")\n",
    "\n",
    "while True:\n",
    "    try:\n",
    "        result = oss_client.search(index=index_name, body={\"query\": {\"match_all\": {}}})\n",
    "        if result['hits']['total']['value'] == len(sampled_shots):\n",
    "            print(\"\\nData is now available for search!\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        print(\".\", end=\"\", flush=True)\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3a8fa9-4d03-47f3-917e-9fc9a93fc736",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"Scott driving a car\"\n",
    "\n",
    "query_embedding = get_embedding('amazon.titan-embed-text-v2:0', user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8bee18-b641-4a08-a9fc-a6c731214630",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_query = {\n",
    "        \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"bool\": {\"should\": []}},\n",
    "                \"script\": {\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"shot_desc_vector\",\n",
    "                        \"query_value\": query_embedding,\n",
    "                        \"space_type\": \"cosinesimil\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"video_path\",\n",
    "            \"shot_id\",\n",
    "            \"shot_startTime\",\n",
    "            \"shot_endTime\",\n",
    "            \"shot_description\",\n",
    "            \"shot_celebrities\",\n",
    "            \"shot_transcript\",\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1323c3c-3dc0-4745-94cf-91db90ab32b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = oss_client.search(body=aoss_query, index=index_name)\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "responses = []\n",
    "for hit in hits:\n",
    "    if hit[\"_score\"] >= 0:  # Set score threshold\n",
    "        responses.append(\n",
    "            {\n",
    "                \"video_path\": hit[\"_source\"][\"video_path\"],\n",
    "                \"shot_id\": hit[\"_source\"][\"shot_id\"],\n",
    "                \"shot_startTime\": hit[\"_source\"][\"shot_startTime\"],\n",
    "                \"shot_endTime\": hit[\"_source\"][\"shot_endTime\"],\n",
    "                \"shot_description\": hit[\"_source\"][\"shot_description\"],\n",
    "                \"shot_celebrities\": hit[\"_source\"][\"shot_celebrities\"],\n",
    "                \"shot_transcript\": hit[\"_source\"][\"shot_transcript\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "651fbe9f-5d65-4bae-87ac-efc9bf17db36",
   "metadata": {},
   "source": [
    "## Shows the top 2 search results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77a1096-0014-42c3-887a-6f8aa5089253",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_with_original_video(top_hit):\n",
    "    \n",
    "    video_path = top_hit['video_path']\n",
    "    video_start = top_hit['shot_startTime']/1000\n",
    "    \n",
    "    display(HTML(f\"\"\"\n",
    "    <video alt=\"test\" controls id=\"{top_hit['shot_id']}\" width=\"100\" >\n",
    "      <source src=\"{video_path}\">\n",
    "    </video>\n",
    "    \n",
    "    <script>\n",
    "    video = document.getElementById(\"{top_hit['shot_id']}\")\n",
    "    video.currentTime = {video_start};\n",
    "    </script>\n",
    "    \"\"\"))\n",
    "    \n",
    "    \n",
    "def display_shot_segemnt_results(response, top_results=2):\n",
    "\n",
    "    css_style = \"\"\"\n",
    "    <style>\n",
    "        .video-container {\n",
    "            display: flex;\n",
    "            justify-content: space-around;\n",
    "            flex-wrap: wrap;\n",
    "        }\n",
    "        .video {\n",
    "            flex: 1;\n",
    "            min-width: 400px;\n",
    "            margin: 10px;\n",
    "        }\n",
    "        video {\n",
    "            width: 100%;\n",
    "            height: auto;\n",
    "        }\n",
    "    </style>\n",
    "    \"\"\"\n",
    "\n",
    "    html_content = \"<div class='video-container'>\\n\"\n",
    "    \n",
    "    for idx in range(top_results):\n",
    "        # convert format of timestamps\n",
    "        video_start = responses[idx]['shot_startTime']/1000\n",
    "        video_end = responses[idx]['shot_endTime']/1000\n",
    "    \n",
    "        converted_start = str(datetime.timedelta(seconds = video_start))\n",
    "        converted_end = str(datetime.timedelta(seconds = video_end))\n",
    "        output_file = f\"shot-{responses[idx]['shot_id']}.mp4\"\n",
    "        _ = subprocess.run(\n",
    "            [\n",
    "                \"/usr/bin/ffmpeg\",\n",
    "                \"-ss\",\n",
    "                converted_start,\n",
    "                \"-to\",\n",
    "                converted_end,\n",
    "                \"-i\",\n",
    "                responses[idx]['video_path'], # path to video\n",
    "                \"-c\",\n",
    "                \"copy\",\n",
    "                output_file,\n",
    "            ],\n",
    "            stderr=subprocess.PIPE\n",
    "        )\n",
    "        html_content += f\"\"\"\n",
    "            <div class=\"video\">\n",
    "                <h5>Shot Id: {responses[idx]['shot_id']}, Time Range: {video_start} ms - {video_end} ms</p>\n",
    "                <video controls>\n",
    "                    <source src=\"{output_file}\" type=\"video/mp4\">\n",
    "                    Your browser does not support the video tag.\n",
    "                </video>\n",
    "            </div>\n",
    "        \"\"\"\n",
    "    # render the shots\n",
    "    html_content += \"</div>\"\n",
    "    \n",
    "    display(HTML(css_style + html_content))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a1c2e0-06ca-4ed3-83c1-9d2f1d898f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"====== [TOP results] =======\", 'green'))\n",
    "display_shot_segemnt_results(responses, top_results=3)\n",
    "\n",
    "print(colored(\"\\n====== [Display top hit as part Of original video] =======\\n\", 'green'))\n",
    "\n",
    "top_hit = responses[0]\n",
    "\n",
    "video_start = top_hit['shot_startTime']/1000\n",
    "video_end = top_hit['shot_endTime']/1000\n",
    "\n",
    "print(f\"Shot Id: {top_hit['shot_id']}, Time Range: {video_start} ms - {video_end} ms\")\n",
    "render_with_original_video(top_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8579f37-81b6-4b45-b30e-d6b5452a0585",
   "metadata": {},
   "source": [
    "### Multi-Model Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5374481-3a36-49d9-9cc5-bcd6967a73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_sample_image(shots):\n",
    "    shot = random.choice(shots) if shots else None\n",
    "    frame_locations = shot['composite_images'][0]['layout']\n",
    "    frame_info = random.choice(frame_locations) if frame_locations else None\n",
    "    return frame_info[0]\n",
    "\n",
    "random_frame = random_sample_image(sampled_shots)\n",
    "image = Image.open(random_frame)\n",
    "image.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a33089-5044-4543-bbf3-cc98d354bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_embedding = get_embedding('amazon.titan-embed-image-v1', random_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a278349e-f96d-4a80-943b-32d8d4fe3c40",
   "metadata": {},
   "outputs": [],
   "source": [
    "aoss_query = {\n",
    "        \"size\": 10,\n",
    "        \"query\": {\n",
    "            \"script_score\": {\n",
    "                \"query\": {\"bool\": {\"should\": []}},\n",
    "                \"script\": {\n",
    "                    \"lang\": \"knn\",\n",
    "                    \"source\": \"knn_score\",\n",
    "                    \"params\": {\n",
    "                        \"field\": \"shot_image_vector\",\n",
    "                        \"query_value\": image_embedding,\n",
    "                        \"space_type\": \"cosinesimil\",\n",
    "                    },\n",
    "                },\n",
    "            }\n",
    "        },\n",
    "        \"_source\": [\n",
    "            \"video_path\",\n",
    "            \"shot_id\",\n",
    "            \"shot_startTime\",\n",
    "            \"shot_endTime\",\n",
    "            \"shot_description\",\n",
    "            \"shot_celebrities\",\n",
    "            \"shot_transcript\",\n",
    "        ],\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9d0858-63a3-449b-9829-a13716487db6",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = oss_client.search(body=aoss_query, index=index_name)\n",
    "hits = response[\"hits\"][\"hits\"]\n",
    "\n",
    "responses = []\n",
    "for hit in hits:\n",
    "    if hit[\"_score\"] >= 0:  # Set score threshold\n",
    "        responses.append(\n",
    "            {\n",
    "                \"video_path\": hit[\"_source\"][\"video_path\"],\n",
    "                \"shot_id\": hit[\"_source\"][\"shot_id\"],\n",
    "                \"shot_startTime\": hit[\"_source\"][\"shot_startTime\"],\n",
    "                \"shot_endTime\": hit[\"_source\"][\"shot_endTime\"],\n",
    "                \"shot_description\": hit[\"_source\"][\"shot_description\"],\n",
    "                \"shot_celebrities\": hit[\"_source\"][\"shot_celebrities\"],\n",
    "                \"shot_transcript\": hit[\"_source\"][\"shot_transcript\"],\n",
    "                \"score\": hit[\"_score\"],\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bc4dc6-c525-4d50-81b1-48b18a30b9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(colored(\"====== [TOP results] =======\", 'green'))\n",
    "display_shot_segemnt_results(responses, top_results=3)\n",
    "\n",
    "print(colored(\"\\n====== [Display top hit as part Of original video] =======\\n\", 'green'))\n",
    "\n",
    "top_hit = responses[0]\n",
    "\n",
    "video_start = top_hit['shot_startTime']/1000\n",
    "video_end = top_hit['shot_endTime']/1000\n",
    "\n",
    "print(f\"Shot Id: {top_hit['shot_id']}, Time Range: {video_start} ms - {video_end} ms\")\n",
    "render_with_original_video(top_hit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289023ae-6fe5-45c9-8420-d161dd8bd1a3",
   "metadata": {},
   "source": [
    "## Clean Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3bd7ce-044e-4a54-a7c3-4b3bedbb9789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try:\n",
    "#     response = oss_client.indices.delete(index=index_name)\n",
    "#     print(f\"Index '{index_name}' deleted successfully\")\n",
    "# except Exception as e:\n",
    "#     print(f\"Error deleting index '{index_name}': {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eac690-12c9-4a69-b66b-52d6b41db5a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
