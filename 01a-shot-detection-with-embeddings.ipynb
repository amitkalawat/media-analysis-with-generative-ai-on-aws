{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f36877-945e-4025-ac91-ff2de0013ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get variables from the previous notebook\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1208d84-6419-473e-97f2-6871de117c43",
   "metadata": {},
   "source": [
    "## Shot Detection\n",
    "As you may recall, a shot is a continuous sequence of frames between two edits or cuts that define one action.  Usually, a shot represents a single camera position, but sometimes shots may contain camera movements such as panning or zooming.  Based on this definition, frames that belong to the same shot should be similar. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43a2226d-1e98-4301-884f-58ffe658c14e",
   "metadata": {},
   "source": [
    "### Create frame embeddings\n",
    "\n",
    "In order to automatically determine where the shots begin and end in the video, we need a way to group similar frames.  In this step, we'll create an embedding for each frame using Amazon Titan Multimodal Embeddings.  Embeddings encode images, text, or both into the same vector space.  Embeddings can be used to cluster these encoded items by similarity.  With embeddings you can implement use cases such as image search, natural language search for images and multi-modal RAG.  We'll be using helper functions in the [lib/frames.py](./lib/frames.py) to accomplish this task.  \n",
    "\n",
    "The calculated frame embeddings will be added to each frame in the `Frames` object that is stored in the `video` variable.\n",
    "\n",
    "Calling the `method make_titan_multimodal_embeddings()` from the `Frames` class will create frame embeddings and store them with the metadata for each frame.  \n",
    "\n",
    "‚è≥ Generating embeddings should take 2-5 minutes.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "If you get an <b>AccessDenied</b> error at this point, make sure you completed the step to enable model access for Amazon Titan Multimodal Embeddings and Anthropic Claude Sonnet 3 in the Amazon Bedrock console.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d210d47-4d42-42ea-8355-8076f9f15bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "video['frames'].make_titan_multimodal_embeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10758689-addb-49fe-b3f7-6afde23c4860",
   "metadata": {},
   "source": [
    "Use the next cell to print the metadata for the first frame and examine the `titan_multimodal_embedding` attribute.   It's a large vector that encodes the content of the frame in the vector space for the `amazon.titan-embed-image-v1` version of the Titan Multimodal Embeddings model.  When we compare this vector with other vectors encoded using the same model version, we can determine if they are similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1696f3-0bd4-4bb4-9638-932b4dd552ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(JSON(video[\"frames\"].frames[0], root=\"first frame\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717706f2-df35-4420-b937-23ada19fc0a0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "video['frames'].display_frames(start=0, end=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9e17b1-72ea-4b5b-bf14-07ecf23fce6a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "In order to compare frames, we need a way to compare embeddings.  We'll implement a [cosine similarity](https://en.wikipedia.org/wiki/Cosine_similarity) function using the Python numpy package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9914d70c-3244-44f9-9a55-3a6e6d0ceb26",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def cosine_similarity(a, b):\n",
    "    cos_sim = dot(a, b) / (norm(a) * norm(b))\n",
    "    return cos_sim"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b9827f-0955-43e2-b6dd-506fc078cd03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Next, let's test comparing some frames.\n",
    "\n",
    "Compare the first black frame to the second frame which is a view of a city street.  As expected, the similarity score is low as these frames are not very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebbad3a-8ff8-407a-9ef5-5f93dc59f7bd",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "frms = video['frames'].frames\n",
    "cosine_similarity(frms[0]['titan_multimodal_embedding'], frms[1]['titan_multimodal_embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b1d1e5b-6638-4a6a-8973-71b5b5771983",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Now, compare the second frame to the third frame.  The similarity score should be higher, since the main difference in these frames is the lettering with the words \"Los Angeles 1947\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a1c6b3-f788-4549-90b6-cd849f4506fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(frms[1]['titan_multimodal_embedding'], frms[2]['titan_multimodal_embedding'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d355fd08-164c-4dbf-9917-04096cdbe4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity(frms[2]['titan_multimodal_embedding'], frms[3]['titan_multimodal_embedding'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a02aa10-8c67-48e5-873c-a42bfc9cd5cc",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Detect shots using Titan Multi-modal Embeddings\n",
    "\n",
    "In this next activity, we'll use the frame embeddings to group similar frames into shots using a pairwise cosine similarity comparison of the embeddings for adjacent frames.  We'll apply the same process we did for the first few frames to all the frames.  We'll choose a SIMILARITY_THRESHOLD to determine if frames are part of the same shot or in different shots.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a456e0f-3ac6-4e99-af0e-290173991e44",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "Execute the code to detect shots in the video.  Based on testing, 0.85 is a good threshold to use for grouping frames for this type of content, but you may need to adjust it to adapt to other situations.  For example, content with more or less action may require different thresholds.  You can try setting it lower or higher to experiment with the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b589e6d7-7a65-4420-ac0c-7e341391194f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "SIMILARITY_THRESHOLD = 0.85\n",
    "\n",
    "video[\"shots\"] = Shots(video[\"frames\"], method=\"SimilarFrames\", min_similarity = SIMILARITY_THRESHOLD)\n",
    "\n",
    "print(f\"Number of shots: {len(video['shots'].shots)} from {len(video['frames'].frames)} frames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3169762a-a662-4a74-a1ad-c5832b565b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
