{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "500968f8",
   "metadata": {},
   "source": [
    "# AI Video Summarization\n",
    "\n",
    "Publishers and broadcasters can leverage short-form video across social media platforms such as Facebook, Instagram, and TikTok to attract new audiences and create additional revenue opportunities.\n",
    "\n",
    "However, generating video summaries is a manual and time-consuming process due to challenges like understanding complex content, maintaining coherence, diverse video types, and lack of scalability when dealing with a large volume of videos. Introducing automation through the use of artificial intelligence (AI) and machine learning (ML) can make this process more viable and scalable with automatic content analysis, real-time processing, contextual adaptation, customization, and continuous AI/ML system improvement.\n",
    "\n",
    "### High level workflow\n",
    "\n",
    "![video summarization diagram](static/images/video-summarization-diagram.jpg)\n",
    "\n",
    "In this notebook, we'll break down each step and show you in detail how video summarization can be achieved using AWS native services such as [Amazon Transcribe](https://aws.amazon.com/pm/transcribe), [Amazon Bedrock](https://aws.amazon.com/bedrock), [Amazon Polly](https://aws.amazon.com/polly/) and [AWS Elemental MediaConvert](https://aws.amazon.com/mediaconvert/)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4657c2",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088571c4",
   "metadata": {},
   "source": [
    "To run this notebook, you need to have run the previous notebook: [01-video-time-segmentation](01-video-time-segmentation.ipynb), where you segmented the video using audio, visual and semantic information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe018e1",
   "metadata": {},
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6202fc75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import boto3\n",
    "import json_repair\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab95822",
   "metadata": {},
   "source": [
    "### Retrieve saved values from previous notebooks\n",
    "To run this notebook, you need to have run the previous notebook: 00_prerequisites.ipynb, where you installed package dependencies and gathered some information from the SageMaker environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f733d62a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e300a9a",
   "metadata": {},
   "source": [
    "## Summarize video content from transcript\n",
    "\n",
    "We use **Large Language Model (LLM)** with [Amazon Bedrock](https://aws.amazon.com/bedrock/) to summarize the content of the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6113bbd-6571-42c3-a0cc-cf9bca4e3410",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(service_name=\"bedrock-runtime\")\n",
    "accept = \"application/json\"\n",
    "content_type = \"application/json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24333345",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(video['transcript'].transcript_file, 'r') as file:\n",
    "    transcript_file = json.load(file)\n",
    "transcript = transcript_file['results']['transcripts'][0]['transcript']\n",
    "\n",
    "model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "\n",
    "prompt = f\"\"\"Summarize the key points from the following video content in chronological order:\n",
    "\n",
    "{transcript} \n",
    "\n",
    "\\n\\nThe summary should only contain information present in the video content. Do not include any new or unrelated information.\n",
    "\n",
    "Important: Start the summary immediately without any introductory phrases. Begin directly with the first key point.\"\"\"\n",
    "\n",
    "body = json.dumps(\n",
    "    {\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": prompt},\n",
    "                ],\n",
    "            }\n",
    "        ],\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.25,\n",
    "        \"top_p\": 0.9,\n",
    "\n",
    "    }\n",
    ")\n",
    "response = bedrock_client.invoke_model(\n",
    "    body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    ")\n",
    "response_body = json.loads(response[\"body\"].read())\n",
    "summarized_text = response_body[\"content\"][0][\"text\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a94157",
   "metadata": {},
   "source": [
    "You can invoke the endpoint with different parameters defined in the payload to impact the text summarization. Two important parameters are `top_p` and `temperature`. While `top_p` is used to control the range of tokens considered by the model based on their cumulative probability, `temperature` controls the level of randomness in the output.\n",
    "\n",
    "Although there isnâ€™t a one-size-fits-all combination of `top_p` and `temperature` for all use cases, in the previous example, we demonstrate sample values with high `top_p` and low `temperature` that leads to summaries focused on key information and avoid deviating from the original text but still introduce some creative variations to keep the output interesting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07258089",
   "metadata": {},
   "source": [
    "Let's check the summarized video content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25fc218",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarized_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da241b73",
   "metadata": {},
   "source": [
    "## Generate metadata for voice narration\n",
    "\n",
    "The next step starts with [Amazon Polly](https://aws.amazon.com/polly/) to generate speech from the summarized text. The output of the Polly task is both MP3 files and documents marked up with [Speech Synthesis Markup Language (SSML)](https://docs.aws.amazon.com/polly/latest/dg/ssml.html). Within this SSML file, essential metadata is encapsulated, describing the duration of individual sentences vocalized by a specific Polly voice. With this audio duration information, we will be able to define the length of the video segments; in this case, a direct 1:1 correspondence is employed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a61487d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "polly_client = boto3.client(\"polly\")\n",
    "voice_id = \"Matthew\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9485e527-5e9c-489d-939b-98e5d1202f42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = polly_client.synthesize_speech(\n",
    "    Engine=\"neural\",\n",
    "    OutputFormat=\"json\",\n",
    "    Text=summarized_text + \" This video is generated by Video Summarization Hub.\",\n",
    "    TextType=\"text\",\n",
    "    SpeechMarkTypes=[\"sentence\"],\n",
    "    VoiceId=voice_id,\n",
    ")\n",
    "\n",
    "stream_data = response['AudioStream'].read()\n",
    "polly_ssml = stream_data.decode('utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5643bba8",
   "metadata": {},
   "source": [
    "The following is the Amazon Polly synthesis task output in SSML format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345f9d0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "polly_ssml.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48aeeb3e",
   "metadata": {},
   "source": [
    "## Select most relevant video shots/scenes\n",
    "\n",
    "We need to select the most relevant video frame sequence to match with every sentence in the summarized content. Thus, we use text embedding to perform the sentence similarity task, which determines how similar two texts are.\n",
    "\n",
    "Sentence similarity models transform input texts into vectors (embeddings) that capture semantic information and calculate the proximity or similarity between them.\n",
    "\n",
    "In this step, we use **Text Embedding Model** with [Amazon Bedrock](https://aws.amazon.com/bedrock/) to create the embeddings for every sentence in the original subtitle and in the video summary.\n",
    "\n",
    "First, we get the original subtitle file and do some processings to break it down into sentences with start times and end times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f0c51a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open(video['transcript'].vtt_file, 'r', encoding='utf-8') as file:\n",
    "    subtitle = file.read()\n",
    "\n",
    "if subtitle.startswith(\"WEBVTT\"):\n",
    "    subtitle = subtitle[len(\"WEBVTT\"):].lstrip()\n",
    "\n",
    "print(subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5bd7bb-aeaa-4cbf-a26c-97f9c19e3091",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def srt_to_arrays(s):\n",
    "    \"\"\"\n",
    "    Converts the given transcription in SRT format to list of sentences and the corresponding timecodes.\n",
    "    Args:\n",
    "       s - transcription in SRT format.\n",
    "    Returns:\n",
    "       a tuple of collection with sentences and the correspoding start time and end time.\n",
    "    \"\"\"\n",
    "    sentences = [line.strip() for line in re.findall(r\"\\d+\\n.*?\\n(.*?)\\n\", s)]\n",
    "\n",
    "    def get_time(s):\n",
    "        return re.findall(r\"\\d{2}:\\d{2}:\\d{2}.\\d{3}\", s)\n",
    "\n",
    "    startTimes = get_time(s)[::2]\n",
    "    endTimes = get_time(s)[1::2]\n",
    "    startTimes_ms = [time_to_ms(time) for time in startTimes]\n",
    "    endTimes_ms = [time_to_ms(time) for time in endTimes]\n",
    "\n",
    "    filtered_sentences = []\n",
    "    filtered_startTimes_ms = []\n",
    "    filtered_endTimes_ms = []\n",
    "\n",
    "    startTime_ms = -1\n",
    "    endTime_ms = -1\n",
    "    sentence = \"\"\n",
    "    for i in range(len(sentences)):\n",
    "        if startTime_ms == -1:\n",
    "            startTime_ms = startTimes_ms[i]\n",
    "        sentence += \" \" + sentences[i]\n",
    "        if (\n",
    "            sentences[i].endswith(\".\")\n",
    "            or sentences[i].endswith(\"?\")\n",
    "            or sentences[i].endswith(\"!\")\n",
    "            or i == len(sentences) - 1\n",
    "        ):\n",
    "            endTime_ms = endTimes_ms[i]\n",
    "            filtered_sentences.append(sentence)\n",
    "            filtered_startTimes_ms.append(startTime_ms)\n",
    "            filtered_endTimes_ms.append(endTime_ms)\n",
    "            startTime_ms = -1\n",
    "            endTime_ms = -1\n",
    "            sentence = \"\"\n",
    "\n",
    "    return filtered_sentences, filtered_startTimes_ms, filtered_endTimes_ms\n",
    "\n",
    "def time_to_ms(time_str):\n",
    "    match = re.match(r\"(\\d+):(\\d+):(\\d+)[.,](\\d+)\", time_str)\n",
    "    h, m, s, ms = match.groups()\n",
    "    return int(h) * 3600000 + int(m) * 60000 + int(s) * 1000 + int(ms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d95579d-84c3-4e12-a3dc-174610a0e6eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_sentences, startTimes, endTimes = srt_to_arrays(subtitle)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7efb3f31-538d-401a-ae82-a827b279ddfb",
   "metadata": {},
   "source": [
    "Let's visualize some the sentences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140ebd7f-e9f7-4b0d-a502-a4530b6d31f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sentences[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2d932e",
   "metadata": {},
   "source": [
    "Next, we create the text embeddings for every sentence in the original subtitle and in the video summary. The following code gives an example of how text embedding using Amazon Bedrock API works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de19961",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def text_embedding(original_sentences, polly_ssml):\n",
    "    \"\"\"\n",
    "    Calculates the similarity between the given original sentences and the summarized sentences in SSML format.\n",
    "    Args:\n",
    "       original_sentences - sentences extacted from the original video\n",
    "       polly_ssml - summarized sentences in SSML format.\n",
    "    Return:\n",
    "       collection of summarized_sentences, corresponding durations in millisecionds, best_matching_indices, similarity_matrix\n",
    "       in a tuple.\n",
    "    \n",
    "    \"\"\"\n",
    "    summarized_sentences = []\n",
    "    durations = []\n",
    "    polly_ssml = polly_ssml.split(\"\\n\")\n",
    "    for i in range(len(polly_ssml) - 1):\n",
    "        curr = polly_ssml[i]\n",
    "        next = polly_ssml[i + 1]\n",
    "        if curr.strip() == \"\" or next.strip() == \"\":\n",
    "            continue\n",
    "        curr = json.loads(curr)\n",
    "        next = json.loads(next)\n",
    "        summarized_sentences.append(curr[\"value\"])\n",
    "        durations.append(int(next[\"time\"]) - int(curr[\"time\"]))\n",
    "\n",
    "    model_id = \"amazon.titan-embed-image-v1\"\n",
    "    accept = \"application/json\"\n",
    "    content_type = \"application/json\"\n",
    "    original_embeddings = []\n",
    "    for str in original_sentences:\n",
    "        body = json.dumps({\"inputText\": str})\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    "        )\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        original_embeddings.append(response_body.get(\"embedding\"))\n",
    "    original_embeddings = np.array(original_embeddings)\n",
    "\n",
    "    summarized_embeddings = []\n",
    "    for str in summarized_sentences:\n",
    "        body = json.dumps({\"inputText\": str})\n",
    "        response = bedrock_client.invoke_model(\n",
    "            body=body, modelId=model_id, accept=accept, contentType=content_type\n",
    "        )\n",
    "        response_body = json.loads(response[\"body\"].read())\n",
    "        summarized_embeddings.append(response_body.get(\"embedding\"))\n",
    "    summarized_embeddings = np.array(summarized_embeddings)\n",
    "\n",
    "    similarity_matrix = np_cosine_similarity(summarized_embeddings, original_embeddings)\n",
    "    best_matching_indices = []\n",
    "    len_summarized_sentences = len(summarized_sentences)\n",
    "    len_original_sentences = len(original_sentences)\n",
    "\n",
    "    # Find the best matching sentences.\n",
    "    dp = np.zeros([len_summarized_sentences, len_original_sentences], dtype=float)\n",
    "    for i in range(0, len_summarized_sentences):\n",
    "        for j in range(0, len_original_sentences):\n",
    "            if i == 0:\n",
    "                dp[i][j] = similarity_matrix[i][j]\n",
    "            else:\n",
    "                max_score = -1\n",
    "                for k in range(0, j):\n",
    "                    if similarity_matrix[i][j] > 0 and dp[i - 1][k] > 0:\n",
    "                        max_score = max(\n",
    "                            max_score, similarity_matrix[i][j] + dp[i - 1][k]\n",
    "                        )\n",
    "                dp[i][j] = max_score\n",
    "\n",
    "    j = len_original_sentences\n",
    "\n",
    "    for i in range(min(len_original_sentences, len_summarized_sentences) - 1, -1, -1):\n",
    "        arr = dp[i][:j]\n",
    "        idx = np.argmax(arr)\n",
    "        best_matching_indices.append(idx)\n",
    "        j = idx\n",
    "    best_matching_indices.reverse()\n",
    "\n",
    "    return summarized_sentences, durations, best_matching_indices, similarity_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c842d841",
   "metadata": {},
   "source": [
    "We use `Cosine similarity` to measure similarities between two vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33e77f25-c887-460b-a4c1-8d5bd6bc5b0d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def np_cosine_similarity(summarized_embeddings, original_embeddings):\n",
    "    dot_products = np.dot(summarized_embeddings, original_embeddings.T)\n",
    "    summarized_norms = np.linalg.norm(summarized_embeddings, axis=1)\n",
    "    original_norms = np.linalg.norm(original_embeddings, axis=1)\n",
    "\n",
    "    similarity_matrix = (\n",
    "        dot_products / summarized_norms[:, None] / original_norms[None, :]\n",
    "    )\n",
    "\n",
    "    return similarity_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a97b08a-4ffa-4a51-a5f4-329210421681",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "summarized_sentences, durations, best_matching_indices, similarity_matrix = text_embedding(original_sentences, polly_ssml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef055d4",
   "metadata": {},
   "source": [
    "This will return the similarity matrix result as follow:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f486a9",
   "metadata": {},
   "source": [
    "You can interpret the prior result as: the first row of the matrix corresponds to the first sentence in the summarized content and all the columns show its similarity scores to the sentences in the original text. Similarity values typically range between -1 and 1, where 1 indicates that the vectors are identical or very similar; 0 indicates that the vectors are orthogonal (not correlated) and have no similarity; -1 indicates that the vectors are diametrically opposed or very dissimilar.\n",
    "\n",
    "From the similarity matrix, we identify the top-k highest similarity scores for each sentence in the summarized content, thereby aligning them with the most similar sentences in the original text. Each sentence in the original text also has its corresponding timestamp (i.e. startTime, endTime) stored in the original SRT subtitle file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b2c77b-bb75-42b0-9d37-4e38ee24524b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_timecodes(best_matching_indices, idx, endTimes, duration, timecodes):\n",
    "    \"\"\"\n",
    "    Calculate the best start and end time for each summarized sentence aligned with the timecode from the original sentences\n",
    "    Args:\n",
    "      best_matching_indices - the indices from the original sentence that is most similar with the summarized sentences.\n",
    "      idx - index from the summarized sentences to process\n",
    "      endTimes - the endtime from the original sentences\n",
    "      duration - speech duration for the synthesized sentences from the summarized text\n",
    "      timecodes - timecode used for calculating the best placement for the summarized text within the video.\n",
    "      \n",
    "    \"\"\"\n",
    "    best_matching_idx = best_matching_indices[idx]\n",
    "    startTime = int(endTimes[best_matching_idx]) - duration\n",
    "    carry = max(0, timecodes[len(timecodes) - 1][1] - startTime)\n",
    "    startTime += carry\n",
    "    endTime = int(endTimes[best_matching_idx]) + carry\n",
    "    return startTime, endTime\n",
    "\n",
    "def milliseconds_to_time(ms, frame_rate=24):\n",
    "    return \"{:02d}:{:02d}:{:02d}:{:02d}\".format(\n",
    "        int((ms // 3600000) % 24),  # hours\n",
    "        int((ms // 60000) % 60),  # minutes\n",
    "        int((ms // 1000) % 60),  # seconds\n",
    "        # fractions of a second (assuming 24 fps)\n",
    "        int((ms % 1000) * frame_rate / 1000),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6690d90e-9cfb-412d-9098-43811db8b53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_time = float(transcript_file[\"results\"][\"items\"][0][\"start_time\"]) * 1000  # ms\n",
    "\n",
    "timecodes = [[0, intro_time]]\n",
    "for i in range(min(len(original_sentences), len(summarized_sentences))):\n",
    "    startTime, endTime = get_timecodes(\n",
    "        best_matching_indices,\n",
    "        i,\n",
    "        endTimes,\n",
    "        durations[i],\n",
    "        timecodes,\n",
    "    )\n",
    "    timecodes.append([startTime, endTime])\n",
    "creditTime = endTime + 3500\n",
    "timecodes.append([endTime, creditTime])\n",
    "timecodes_text = \"\"\n",
    "for timecode in timecodes:\n",
    "    timecodes_text += (\n",
    "        milliseconds_to_time(timecode[0])\n",
    "        + \",\"\n",
    "        + milliseconds_to_time(timecode[1])\n",
    "        + \"\\n\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030714ef-a2bd-46e8-b895-a5d29785cfac",
   "metadata": {},
   "source": [
    "An example of the timestamp output in text format is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a603b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(timecodes_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60211f52-1236-466b-ad64-d841ab549951",
   "metadata": {},
   "source": [
    "In order to make it work with the next video transcoding process with AWS Elemental Convert, we re-format the timecodes as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8465386f-720e-4f0b-ace7-44bc85e0623a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "to_json = lambda s: [\n",
    "    {\"StartTimecode\": t1, \"EndTimecode\": t2}\n",
    "    for t1, t2 in (line.split(\",\") for line in s.split(\"\\n\") if line.strip())\n",
    "]\n",
    "timecodes = to_json(timecodes_text)\n",
    "print(timecodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8f9d01-2b2c-40bf-b798-0655f50a25aa",
   "metadata": {},
   "source": [
    "By incorporating both the duration of Polly audio for each summarized sentence and the timestamps from the original subtitle file, we can then select the timestamp sequence for the most relevant frames corresponding to each summarized sentence. The length of each selected video segment for a summarized sentence will be aligned with the length of its narration audio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e7972d-f9d5-4bdf-8a18-be1e7cea87d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "escaped_summarized_text = (\n",
    "        summarized_text.replace(\"&\", \"&amp;\")\n",
    "        .replace('\"', \"&quot;\")\n",
    "        .replace(\"'\", \"&apos;\")\n",
    "        .replace(\"<\", \"&lt;\")\n",
    "        .replace(\">\", \"&gt;\")\n",
    "    )\n",
    "ssml = \"<speak>\\n\"\n",
    "break_time = intro_time\n",
    "\n",
    "while break_time > 10000:  # maximum break time in Polly is 10s\n",
    "    ssml += '<break time = \"' + str(break_time) + 'ms\"/>'\n",
    "    break_time -= 10000\n",
    "ssml += '<break time = \"' + str(break_time) + 'ms\"/>'\n",
    "ssml += escaped_summarized_text\n",
    "ssml += \"</speak>\"\n",
    "\n",
    "response = polly_client.synthesize_speech(\n",
    "    Engine=\"neural\",\n",
    "    OutputFormat=\"mp3\",\n",
    "    Text=ssml,\n",
    "    TextType=\"ssml\",\n",
    "    VoiceId=voice_id,\n",
    ")\n",
    "\n",
    "if \"AudioStream\" in response:\n",
    "    with response[\"AudioStream\"] as stream:\n",
    "        audio_narration = stream.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf16ae98-439f-4211-9ff8-550f6bf77d06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(ssml)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff4f72f3-bbad-4d89-ad4f-afa62e06fa83",
   "metadata": {},
   "source": [
    "Again, we upload the audio narration into Amazon S3 bucket ready for the videotranscoding step with AWS Elemental Convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab1d74-942d-419a-a9e8-e7c9facf3013",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "s3_bucket = session[\"bucket\"]\n",
    "audio_narration_filename = os.path.splitext(os.path.basename(video['mp4_file']))[0] + \".mp3\"\n",
    "s3_client.put_object(\n",
    "    Body=audio_narration, Bucket=s3_bucket, Key=audio_narration_filename, ContentType=\"audio/mpeg\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4046c3b2",
   "metadata": {},
   "source": [
    "## Create MediaConvert assembly workflows\n",
    "\n",
    "We use the sequence of the timestamps as parameters to create AWS Elemental MediaConvert assembly workflows to performs basic input clipping.\n",
    "\n",
    "By combining it with the MP3 audio from Amazon Polly and along with the possibility of incorporating background music of your preference, you can ultimately achieve the final video summarization output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e622269-11c1-42ce-9318-7d9c2f7b0c5c",
   "metadata": {
    "tags": []
   },
   "source": [
    "Let's start the assembly workflow from the original video input. An assembly workflow is a MediaConvert job that performs basic input clipping and stitching to assemble output assets from one or different sources without requiring separate editing software."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac99a505-65b2-4fbe-a932-07f5d3bdb631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iam_role = session[\"MediaConvertRole\"]\n",
    "input_video_path = video[\"url\"]\n",
    "output_video_path = f\"s3://{s3_bucket}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f8db3-2022-4aae-8baa-a25243105a5c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "media_convert = boto3.client(\"mediaconvert\")\n",
    "response = media_convert.create_job(\n",
    "    Queue=\"Default\",\n",
    "    UserMetadata={},\n",
    "    Role=iam_role,\n",
    "    Settings={\n",
    "        \"TimecodeConfig\": {\"Source\": \"ZEROBASED\"},\n",
    "        \"OutputGroups\": [\n",
    "            {\n",
    "                \"Name\": \"File Group\",\n",
    "                \"Outputs\": [\n",
    "                    {\n",
    "                        \"ContainerSettings\": {\n",
    "                            \"Container\": \"MP4\",\n",
    "                            \"Mp4Settings\": {},\n",
    "                        },\n",
    "                        \"VideoDescription\": {\n",
    "                            \"CodecSettings\": {\n",
    "                                \"Codec\": \"H_264\",\n",
    "                                \"H264Settings\": {\n",
    "                                    \"MaxBitrate\": 40000000,\n",
    "                                    \"RateControlMode\": \"QVBR\",\n",
    "                                    \"SceneChangeDetect\": \"TRANSITION_DETECTION\",\n",
    "                                },\n",
    "                            }\n",
    "                        }\n",
    "                    }\n",
    "                ],\n",
    "                \"OutputGroupSettings\": {\n",
    "                    \"Type\": \"FILE_GROUP_SETTINGS\",\n",
    "                    \"FileGroupSettings\": {\"Destination\": output_video_path},\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"Inputs\": [\n",
    "            {\n",
    "                \"VideoSelector\": {},\n",
    "                \"TimecodeSource\": \"ZEROBASED\",\n",
    "                \"FileInput\": input_video_path,\n",
    "                \"InputClippings\": timecodes,\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    AccelerationSettings={\"Mode\": \"DISABLED\"},\n",
    "    StatusUpdateInterval=\"SECONDS_60\",\n",
    "    Priority=0,\n",
    ")\n",
    "\n",
    "job_complete = False\n",
    "\n",
    "while not job_complete:\n",
    "    job_response = media_convert.get_job(Id=response[\"Job\"][\"Id\"])\n",
    "    \n",
    "    job_status = job_response['Job']['Status']\n",
    "    print(f\"MediaConvert job status: {job_status}\")\n",
    "    \n",
    "    if job_status == 'COMPLETE':\n",
    "        print(\"Job is complete!\")\n",
    "        job_complete = True\n",
    "    elif job_status == 'ERROR':\n",
    "        print(\"Job has failed.\")\n",
    "        job_complete = True\n",
    "    else:\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a672bd8-f338-49d2-a4d1-7387aca9ef83",
   "metadata": {},
   "source": [
    "Finally, you create audio tracks in the output and associate a single audio selector with each output track. In addition, you could also add a subtitle into the final video ouput. You could generate a subtitle for the video summary as follow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cf7378-b0a4-429d-a2f3-b29bb2ec6ac2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "video_summary_subtitle = \"\"\n",
    "start = intro_time\n",
    "\n",
    "def split_long_lines(text, max_line_length):\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_length = 0\n",
    "\n",
    "    for word in words:\n",
    "        if current_length + len(word) + len(current_line) > max_line_length:\n",
    "            lines.append(\" \".join(current_line))\n",
    "            current_line = []\n",
    "            current_length = 0\n",
    "        current_line.append(word)\n",
    "        current_length += len(word) + 1\n",
    "\n",
    "    if current_line:\n",
    "        lines.append(\" \".join(current_line))\n",
    "\n",
    "    return lines\n",
    "\n",
    "def milliseconds_to_subtitleTimeFormat(ms):\n",
    "    return \"{:02d}:{:02d}:{:02d},{:03d}\".format(\n",
    "        int((ms // 3600000) % 24),  # hours\n",
    "        int((ms // 60000) % 60),  # minutes\n",
    "        int((ms // 1000) % 60),  # seconds\n",
    "        int(ms % 1000),  # milliseconds\n",
    "    )\n",
    "\n",
    "for i in range(min(len(original_sentences), len(summarized_sentences))):\n",
    "    end = start + durations[i]\n",
    "    video_summary_subtitle += f\"{i+1}\\n\"\n",
    "    video_summary_subtitle += f\"{milliseconds_to_subtitleTimeFormat(start)} --> {milliseconds_to_subtitleTimeFormat(end)}\\n\"\n",
    "    sentence_lines = split_long_lines(summarized_sentences[i], 90)\n",
    "    for line in sentence_lines:\n",
    "        video_summary_subtitle += f\"{line}\\n\"\n",
    "    video_summary_subtitle += \"\\n\"\n",
    "    start = end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cee4cf-cc78-4948-8c82-decdc68994cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(video_summary_subtitle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52990b82-c585-4aed-87c9-dfd8026146cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "subtitle_filename = os.path.splitext(os.path.basename(video['mp4_file']))[0] + \".srt\"\n",
    "s3_client.put_object(\n",
    "    Body=video_summary_subtitle, Bucket=s3_bucket, Key=subtitle_filename\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab15bb2-d99c-4c68-8b24-8b3bc32d3fe1",
   "metadata": {},
   "source": [
    "Finally, you create a MediaConvert job for the final video ouput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27cc265f-19d2-4e79-88bc-00d0c19877a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "input_video_path = f\"s3://{s3_bucket}/{video['mp4_file']}\"\n",
    "audio_file_path = f\"s3://{s3_bucket}/{audio_narration_filename}\"\n",
    "subtitle_file_path = f\"s3://{s3_bucket}/{subtitle_filename}\"\n",
    "output_video_path = f\"s3://{s3_bucket}/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba312ef-46a2-4166-ba5b-61a613ba94f3",
   "metadata": {},
   "source": [
    "In the following step, we are using a [AWS Elemental MediaConvert](https://aws.amazon.com/mediaconvert/) job to apply the narrated voice and the subtitles on the original video. The output is written to S3 bucket for downstream consumption. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8ec775-5ad9-4ab8-a95b-5b3090d40b55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "response = media_convert.create_job(\n",
    "    Queue=\"Default\",\n",
    "    UserMetadata={},\n",
    "    Role=iam_role,\n",
    "    Settings={\n",
    "        \"TimecodeConfig\": {\"Source\": \"ZEROBASED\"},\n",
    "        \"OutputGroups\": [\n",
    "            {\n",
    "                \"Name\": \"File Group\",\n",
    "                \"Outputs\": [\n",
    "                    {\n",
    "                        \"ContainerSettings\": {\n",
    "                            \"Container\": \"MP4\",\n",
    "                            \"Mp4Settings\": {},\n",
    "                        },\n",
    "                        \"VideoDescription\": {\n",
    "                            \"CodecSettings\": {\n",
    "                                \"Codec\": \"H_264\",\n",
    "                                \"H264Settings\": {\n",
    "                                    \"MaxBitrate\": 40000000,\n",
    "                                    \"RateControlMode\": \"QVBR\",\n",
    "                                    \"SceneChangeDetect\": \"TRANSITION_DETECTION\",\n",
    "                                },\n",
    "                            }\n",
    "                        },\n",
    "                        \"NameModifier\": \"_summary\",\n",
    "                        \"AudioDescriptions\": [\n",
    "                            {\n",
    "                                \"AudioSourceName\": \"Audio Selector Group 1\",\n",
    "                                \"CodecSettings\": {\n",
    "                                    \"Codec\": \"AAC\",\n",
    "                                    \"AacSettings\": {\n",
    "                                        \"Bitrate\": 96000,\n",
    "                                        \"CodingMode\": \"CODING_MODE_2_0\",\n",
    "                                        \"SampleRate\": 48000,\n",
    "                                    },\n",
    "                                },\n",
    "                            }\n",
    "                        ],\n",
    "                        \"CaptionDescriptions\": [\n",
    "                            {\n",
    "                                \"CaptionSelectorName\": \"Captions Selector 1\",\n",
    "                                \"DestinationSettings\": {\n",
    "                                    \"DestinationType\": \"BURN_IN\",\n",
    "                                    \"BurninDestinationSettings\": {\n",
    "                                        \"BackgroundOpacity\": 100,\n",
    "                                        \"FontSize\": 18,\n",
    "                                        \"FontColor\": \"WHITE\",\n",
    "                                        \"ApplyFontColor\": \"ALL_TEXT\",\n",
    "                                        \"BackgroundColor\": \"BLACK\",\n",
    "                                    },\n",
    "                                },\n",
    "                            }\n",
    "                        ],\n",
    "                    }\n",
    "                ],\n",
    "                \"OutputGroupSettings\": {\n",
    "                    \"Type\": \"FILE_GROUP_SETTINGS\",\n",
    "                    \"FileGroupSettings\": {\"Destination\": output_video_path},\n",
    "                },\n",
    "            }\n",
    "        ],\n",
    "        \"Inputs\": [\n",
    "            {\n",
    "                \"AudioSelectors\": {\n",
    "                    \"Audio Selector 1\": {\n",
    "                        \"DefaultSelection\": \"NOT_DEFAULT\",\n",
    "                        \"ExternalAudioFileInput\": audio_file_path,\n",
    "                    },\n",
    "                },\n",
    "                \"AudioSelectorGroups\": {\n",
    "                    \"Audio Selector Group 1\": {\n",
    "                        \"AudioSelectorNames\": [\"Audio Selector 1\"]\n",
    "                    }\n",
    "                },\n",
    "                \"VideoSelector\": {},\n",
    "                \"TimecodeSource\": \"ZEROBASED\",\n",
    "                \"CaptionSelectors\": {\n",
    "                    \"Captions Selector 1\": {\n",
    "                        \"SourceSettings\": {\n",
    "                            \"SourceType\": \"SRT\",\n",
    "                            \"FileSourceSettings\": {\"SourceFile\": subtitle_file_path},\n",
    "                        }\n",
    "                    }\n",
    "                },\n",
    "                \"FileInput\": input_video_path,\n",
    "            }\n",
    "        ],\n",
    "    },\n",
    "    AccelerationSettings={\"Mode\": \"DISABLED\"},\n",
    "    StatusUpdateInterval=\"SECONDS_60\",\n",
    "    Priority=0,\n",
    ")\n",
    "\n",
    "job_complete = False\n",
    "\n",
    "while not job_complete:\n",
    "    job_response = media_convert.get_job(Id=response[\"Job\"][\"Id\"])\n",
    "    \n",
    "    job_status = job_response['Job']['Status']\n",
    "    print(f\"MediaConvert job status: {job_status}\")\n",
    "    \n",
    "    if job_status == 'COMPLETE':\n",
    "        print(\"Job is complete!\")\n",
    "        job_complete = True\n",
    "    elif job_status == 'ERROR':\n",
    "        print(\"Job has failed.\")\n",
    "        job_complete = True\n",
    "    else:\n",
    "        time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8189efa",
   "metadata": {},
   "source": [
    "## Short-form video output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637926aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "video_summary = os.path.splitext(os.path.basename(video['mp4_file']))[0] + \"_summary.mp4\"\n",
    "s3_client.download_file(s3_bucket, video_summary, video_summary)\n",
    "Video(url=video_summary, width=640, height=360, html_attributes=\"controls muted autoplay\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be69f6e3-d26e-4aac-b09f-cecaee7c9b99",
   "metadata": {},
   "source": [
    "## Clean up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2189d57-fe0e-4ce5-85b6-95ee9a0a82f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# s3_client.delete_object(Bucket=s3_bucket, Key=audio_narration_filename)\n",
    "# s3_client.delete_object(Bucket=s3_bucket, Key=video['mp4_file'])\n",
    "# s3_client.delete_object(Bucket=s3_bucket, Key=subtitle_filename)\n",
    "# s3_client.delete_object(Bucket=s3_bucket, Key=video_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e43a50b-8b30-4d54-bf01-e4c6bd98c332",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
