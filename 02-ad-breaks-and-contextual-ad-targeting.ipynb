{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c0b82119-b336-4f19-af09-56827ed617af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Ad break detection and contextual Ad targeting\n",
    "\n",
    "Contextual advertising is a form of targeted advertising where the advertisement is matched to the context of the webpage or media being consumed by the user. This process involves three key players: the publisher (website or content owner), the advertiser, and the consumer. Publishers provide the platform and content, while advertisers create ads tailored to the context. Consumers engage with the content, and relevant ads are displayed based on the context, creating a more personalized and relevant advertising experience.\n",
    "\n",
    "A challenging area of contextual advertising is inserting ads in media content for streaming on video on demand (VOD) platforms. This process traditionally relied on manual tagging, where human experts analyzed the content, identified breaks in the narrative and assigned relevant keywords or categories. However, this approach is time-consuming, subjective, and may not capture the full context or nuances of the content. Traditional AI/ML solutions can automate this process, but they often require extensive training data and can be expensive and limited in their capabilities.\n",
    "\n",
    "Generative AI, powered by large language models, offers a promising solution to this challenge. By leveraging the vast knowledge and contextual understanding of these models, publishers can automatically generate contextual insights and taxonomies for their media assets. This approach streamlines the process and provides accurate and comprehensive contextual understanding, enabling effective ad targeting and monetization of media archives.\n",
    "\n",
    "When you are done with this part of the workshop, you'll have created the following metadata for a video:\n",
    "* a list of high quality ad placement opportunities  or _breaks_ available in the video\n",
    "* contextual information for the video before and after each break, including classification using the IAB Content Taxonomy that is used by advertisers to classify content for automated placement using Ad Decision Servers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca97816-35b5-40c4-b194-ba88c68b0222",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88dadf3d-559a-4a5a-9e9d-509f9139a0b5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "To run this notebook, you need to have run the previous notebook:[01-video-time-segmentation](01-video-time-segmentation.ipynb), where you segmented the video using audio, visual and semantic information."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432f3368-b3ea-406d-bc46-328e3e322d8f",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Import python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9d4c51-6580-4386-a398-df098834657b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import json\n",
    "import json\n",
    "import boto3\n",
    "import json_repair\n",
    "import copy\n",
    "from termcolor import colored\n",
    "from IPython.display import JSON\n",
    "from IPython.display import Video\n",
    "from IPython.display import Pretty\n",
    "from IPython.display import Image as DisplayImage\n",
    "from lib.frames import VideoFrames\n",
    "from lib.shots import Shots\n",
    "from lib.scenes import Scenes\n",
    "from lib.transcript import Transcript\n",
    "#from lib.chapters import Chapters\n",
    "from lib import bedrock_helper as brh\n",
    "from lib import frame_utils\n",
    "from lib import util"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2636a0da-4572-4c3b-8b80-4ee2ed465181",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Retrieve saved values from previous notebooks\n",
    "To run this notebook, you need to have run the previous notebook: 00_prerequisites.ipynb, where you installed package dependencies and gathered some information from the SageMaker environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d96940dd-7733-4a7d-8782-8c7779f15432",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25bb30cd-62be-40e5-8370-db452a029a06",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "\n",
    "## Find ad placement opportunities by aligning scenes and topics to identify chapters in the narrative\n",
    "\n",
    "In the [Video segmentation notebook](video-understanding-with-generative-ai-on-aws-main/01-video-time-segmentation.ipynb), we have separately processed the visual and audio cues from the video. Now, we will do one more step to bring them together and ensure that the transcription topics align with the scenes. The last thing you want is to insert an ad during an ongoing conversation or scene. To create alignment, we will iterate over each conversational topic, represented by its start and end timestamps, and a text description summarizing the topic. For each topic, the code identifies the relevant video scenes that overlap or fall within the topic's timestamp range. The output of this process is a list of chapters, where each chapter contains a list of scene IDs representing the video scenes that align with the corresponding audio conversation. After the alignment process, we have combined visual and audio cues into the final chapters. The breaks between chapters are ideal places for ad insertion because they occur between contextual changes in the content of the video. \n",
    "\n",
    "In real-world applications, we recommend surfacing these breaks as suggestions to the operator and having a human-in-the-loop step to confirm the final ad placements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9437b9ed-1bb4-4522-8e47-249388ac7719",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "from lib import frame_utils\n",
    "import os\n",
    "\n",
    "class Chapters:\n",
    "    def __init__(self, topics, scenes, frames):\n",
    "        self.video_asset_dir = frames.video_asset_dir()\n",
    "        self.chapters = self.align_scenes_in_chapters(topics, scenes, frames)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def align_scenes_in_chapters(self, topics, scenes, frames):\n",
    "        scenes = copy.deepcopy(scenes)\n",
    "    \n",
    "        chapters = []\n",
    "        for topic in topics:\n",
    "            print(f\"Topic: {topic['id']}\")\n",
    "            start_ms = topic['start_ms']\n",
    "            end_ms = topic['end_ms']\n",
    "            text = topic['reason']\n",
    "\n",
    "            # find all the frames that align with the conversation topic\n",
    "            stack = []\n",
    "            while len(scenes) > 0:\n",
    "                scene = scenes[0]\n",
    "                frame_start = scene['start_ms']\n",
    "                frame_end = scene['end_ms']\n",
    "    \n",
    "                if frame_start > end_ms:\n",
    "                    break\n",
    "    \n",
    "                # scenes before any conversation starts\n",
    "                if frame_end < start_ms:\n",
    "                    chapter = Chapter(len(chapters), [scene], frames).__dict__\n",
    "                    chapters.append(chapter)\n",
    "                    scenes.pop(0)\n",
    "                    continue\n",
    "    \n",
    "                stack.append(scene)\n",
    "                scenes.pop(0)\n",
    "    \n",
    "            if stack:\n",
    "                chapter = Chapter(len(chapters), stack, frames, text).__dict__\n",
    "                chapters.append(chapter)\n",
    "    \n",
    "        ## There could be more scenes without converations, append them\n",
    "        for scene in scenes:\n",
    "            chapter = Chapter(len(chapters), [scene], frames).__dict__\n",
    "            chapters.append(chapter)\n",
    "    \n",
    "        return chapters\n",
    "\n",
    "class Chapter:\n",
    "    def __init__(self, chapter_id, scenes, frames, text = ''):\n",
    "        self.scene_ids = [scene['id'] for scene in scenes]\n",
    "        self.start_frame_id = scenes[0]['start_frame_id']\n",
    "        self.end_frame_id = scenes[-1]['end_frame_id']\n",
    "        self.start_ms = scenes[0]['start_ms']\n",
    "        self.end_ms = scenes[-1]['end_ms']\n",
    "        self.id = chapter_id\n",
    "        self.text = text\n",
    "        self.composite_images = self.create_composite_images(frames.frames[self.start_frame_id:self.end_frame_id+1], frames.video_asset_dir())\n",
    "        \n",
    "        return \n",
    "\n",
    "    def create_composite_images(self, frames, video_asset_dir):\n",
    "        folder = os.path.join(video_asset_dir, 'chapters')\n",
    "        os.makedirs(folder, exist_ok=True) \n",
    "        composite_images = frame_utils.create_composite_images(\n",
    "                frames,\n",
    "                output_dir = folder,\n",
    "                prefix = 'chapter_',\n",
    "                max_dimension = (1568, 1568),\n",
    "                burn_timecode = False\n",
    "                )\n",
    "        return composite_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1339c1bf-6ade-4a05-868d-069424bccdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "video['chapters'] = Chapters(video['topics'], video['scenes'].scenes, video['frames'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450dbb89-c2ad-43d9-8c4b-fefaf00ed38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(JSON(video['scenes'].__dict__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f30eb92-f37f-46f5-98b7-1d238056785a",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(JSON(video['chapters'].__dict__))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3b4ffc-bc67-4a1f-8749-362815e1154a",
   "metadata": {},
   "source": [
    "#### Visualize the chapters\n",
    "\n",
    "Now let's visualize some chapters using the generated composite images. Note that some chapters will have more than one composite image.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "💡 Use the scroll bar in the output box to view the chapters.  Some chapters contain more frames than can fit on a single composite image, so the may be multiple composite images displayed for each chapter.\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6036728d-ea60-47a3-9167-d2f5949f9b96",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the chapters\n",
    "\n",
    "STOP=10\n",
    "for counter, b in enumerate(video[\"chapters\"].chapters):\n",
    "    print(f'\\nChapter {counter}: frames {b[\"start_frame_id\"] } to {b[\"end_frame_id\"] } =======\\n')\n",
    "    for image_file in b['composite_images']:\n",
    "        print(image_file['file'])\n",
    "        display(DisplayImage(filename=image_file['file'], height=200))\n",
    "        #display(Image('image.png', height=400))\n",
    "    if counter == STOP:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e484ed-cf3c-41a5-9bab-771c56fa47f6",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Generate topic level contextual information \n",
    "\n",
    "The last step is to send both the visually and audio-aligned data to Claude 3 Haiku to generate contextual information for each topic. This approach that takes advantage of the multimodal capabilities of the Claude 3 family of models. From our testing, these models have demonstrated the ability to capture minute details from large images and follow image sequences when provided with appropriate instructions.\n",
    "\n",
    "To prepare the input for Claude3 Haiku, we first assemble video frames associated with each topic and create a composite image grid. Through our experimentation, we have found that the optimum image grid ratio is 7 rows by 4 columns, which will assemble a 1568 x 1540 pixel image that fits under Claude's 5 MB image file size limit while still preserving enough detail in each individual frame tile. Furthermore, you can also assemble multiple images if needed.\n",
    "\n",
    "Subsequently, the composite images, the transcription, the IAB Content taxonomy definitions, and GARM taxonomy definitions are fed into the prompt to generate descriptions, sentiment, IAB taxonomy, GARM taxonomy, and other relevant information in a single query to the Claude3 Haiku model. Not only that, but we can adapt this approach to any taxonomy or custom labeling use cases without the need to train a model each time. This is where the true power of this approach lies. The final output can be presented to a human reviewer for final confirmation if needed. Here is an example of a composite image grid and the corresponding contextual output for a specific topic.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e01a83-55cf-4ab7-944c-a6c9d3448414",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Download the IAB Content Taxonomy definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77fdff3d-466b-4d0e-8f3e-6df0188f29e5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "iab_file = 'iab_content_taxonomy_v3.json'\n",
    "url = f\"https://dx2y1cac29mt3.cloudfront.net/iab/{iab_file}\"\n",
    "\n",
    "!curl {url} -o {iab_file}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "514bd101-affd-4630-a192-bee997980abb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_iab_taxonomies(file):\n",
    "    with open(file) as f:\n",
    "        iab_taxonomies = json.load(f)\n",
    "    return iab_taxonomies\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efdb888-fb4b-41ea-b85f-59be51f89e93",
   "metadata": {},
   "source": [
    "### Generate contextual metadata for each chapter segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99effde5-caba-4d24-9199-7346042c34c9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_usage = {\n",
    "    'input_tokens': 0,\n",
    "    'output_tokens': 0,\n",
    "}\n",
    "\n",
    "iab_definitions = load_iab_taxonomies(iab_file)\n",
    "\n",
    "for chapter in video['chapters'].chapters:\n",
    "\n",
    "    composite_images = chapter['composite_images']\n",
    "    num_images = len(composite_images)\n",
    "\n",
    "    contextual_response = brh.get_contextual_information(composite_images, chapter['text'], iab_definitions)\n",
    "\n",
    "    usage = contextual_response['usage']\n",
    "    contextual = contextual_response['content'][0]['json']\n",
    "\n",
    "    # save the contextual to the chapter\n",
    "    chapter['contextual'] = {\n",
    "        'usage': usage,\n",
    "        **contextual\n",
    "    }\n",
    "    \n",
    "    total_usage['input_tokens'] += usage['input_tokens']\n",
    "    total_usage['output_tokens'] += usage['output_tokens']\n",
    "\n",
    "    print(f\"==== Chapter #{chapter['id']:02d}: Contextual information ======\")\n",
    "    for key in ['description', 'sentiment', 'iab_taxonomy', 'garm_taxonomy']:\n",
    "        print(f\"{key.capitalize()}: {colored(contextual[key]['text'], 'green')} ({contextual[key]['score']}%)\")\n",
    "\n",
    "    for key in ['brands_and_logos', 'relevant_tags']:\n",
    "        items = ', '.join([item['text'] for item in contextual[key]])\n",
    "        if len(items) == 0:\n",
    "            items = 'None'\n",
    "        print(f\"{key.capitalize()}: {colored(items, 'green')}\")\n",
    "    print(f\"================================================\\n\\n\")\n",
    "\n",
    "output_file = os.path.join(video[\"video_dir\"], 'scenes_in_chapters.json')\n",
    "util.save_to_file(output_file, video['chapters'].chapters)\n",
    "\n",
    "contextual_cost = brh.display_contextual_cost(total_usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65d1881-87f6-4ab0-a177-97370075bfab",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "JSON(video['chapters'].chapters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40ec109-e2a7-44a4-b1b9-2636fd518046",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Visualize the ad breaks between chapters\n",
    "* use video.py to make a video?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a550374b-8285-459a-a138-247fa0899993",
   "metadata": {},
   "outputs": [],
   "source": [
    "JSON(video['shots'].shots[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ddc979-bf0f-4aca-a0a8-72147b95aeb4",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import moviepy\n",
    "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
    "\n",
    "adbreak_start = video['chapters'].chapters[9]['start_ms']/1000\n",
    "\n",
    "clip1 = VideoFileClip(\"Netflix_Open_Content_Meridian.mp4\").subclip(adbreak_start-10, adbreak_start)\n",
    "clip2 = VideoFileClip(\"static/images/CountdownClock_0.mp4\")\n",
    "clip3 = VideoFileClip(\"Netflix_Open_Content_Meridian.mp4\").subclip(adbreak_start, adbreak_start+10)\n",
    "final_clip = concatenate_videoclips([clip1,clip2,clip3], method=\"compose\")\n",
    "final_clip.write_videofile(\"ad_break_demo.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8181640e-de93-4f58-8d87-d82bc49ce33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "Video(url='ad_break_demo.mp4', width=640, height=360)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
